{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8329940,"sourceType":"datasetVersion","datasetId":4946147},{"sourceId":8393115,"sourceType":"datasetVersion","datasetId":4992831},{"sourceId":8544070,"sourceType":"datasetVersion","datasetId":5104702}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Brain teaser","metadata":{}},{"cell_type":"code","source":"!pip install -q -U langchain transformers bitsandbytes accelerate","metadata":{"execution":{"iopub.status.busy":"2024-06-04T18:52:36.594209Z","iopub.execute_input":"2024-06-04T18:52:36.594559Z","iopub.status.idle":"2024-06-04T18:53:20.382009Z","shell.execute_reply.started":"2024-06-04T18:52:36.594535Z","shell.execute_reply":"2024-06-04T18:53:20.380875Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncudf 23.8.0 requires cubinlinker, which is not installed.\ncudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ncudf 23.8.0 requires ptxcompiler, which is not installed.\ncuml 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ndask-cudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\nkeras-cv 0.8.2 requires keras-core, which is not installed.\nkeras-nlp 0.9.3 requires keras-core, which is not installed.\ntensorflow-decision-forests 1.8.1 requires wurlitzer, which is not installed.\napache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.8 which is incompatible.\napache-beam 2.46.0 requires numpy<1.25.0,>=1.14.3, but you have numpy 1.26.4 which is incompatible.\napache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 15.0.2 which is incompatible.\ncudf 23.8.0 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.4.0 which is incompatible.\ncudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.1.4 which is incompatible.\ncudf 23.8.0 requires protobuf<5,>=4.21, but you have protobuf 3.20.3 which is incompatible.\ncudf 23.8.0 requires pyarrow==11.*, but you have pyarrow 15.0.2 which is incompatible.\ncuml 23.8.0 requires dask==2023.7.1, but you have dask 2024.4.1 which is incompatible.\ndask-cuda 23.8.0 requires dask==2023.7.1, but you have dask 2024.4.1 which is incompatible.\ndask-cuda 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.1.4 which is incompatible.\ndask-cudf 23.8.0 requires dask==2023.7.1, but you have dask 2024.4.1 which is incompatible.\ndask-cudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.1.4 which is incompatible.\ndistributed 2023.7.1 requires dask==2023.7.1, but you have dask 2024.4.1 which is incompatible.\ngoogle-cloud-bigquery 2.34.4 requires packaging<22.0dev,>=14.3, but you have packaging 23.2 which is incompatible.\njupyterlab 4.1.6 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\njupyterlab-lsp 5.1.0 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\nlibpysal 4.9.2 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\nmomepy 0.7.0 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\nosmnx 1.9.2 requires shapely>=2.0, but you have shapely 1.8.5.post1 which is incompatible.\nraft-dask 23.8.0 requires dask==2023.7.1, but you have dask 2024.4.1 which is incompatible.\nspopt 0.6.0 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\ntensorflow 2.15.0 requires keras<2.16,>=2.15.0, but you have keras 3.2.1 which is incompatible.\nydata-profiling 4.6.4 requires numpy<1.26,>=1.16.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np\nimport json\nfrom datasets import load_dataset\nfrom random import shuffle\nimport random\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\n\nimport torch\nfrom torch.nn.functional import normalize\nfrom transformers import AutoModel, AutoTokenizer\n\nfrom tqdm.auto import tqdm\nimport numpy as np\nimport os\nfrom transformers import StoppingCriteriaList\n\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline\nimport torch","metadata":{"execution":{"iopub.status.busy":"2024-06-04T18:53:20.383952Z","iopub.execute_input":"2024-06-04T18:53:20.384293Z","iopub.status.idle":"2024-06-04T18:53:48.289529Z","shell.execute_reply.started":"2024-06-04T18:53:20.384262Z","shell.execute_reply":"2024-06-04T18:53:48.288679Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"2024-06-04 18:53:32.360458: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-06-04 18:53:32.360590: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-06-04 18:53:32.605514: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"# Ako imamo vise ponudenih odgovora od 4, da postupak bude automatski u svim funkcijama\nletters = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P'][:4]\nletters, len(letters)","metadata":{"execution":{"iopub.status.busy":"2024-06-04T18:53:48.290640Z","iopub.execute_input":"2024-06-04T18:53:48.291242Z","iopub.status.idle":"2024-06-04T18:53:48.300900Z","shell.execute_reply.started":"2024-06-04T18:53:48.291213Z","shell.execute_reply":"2024-06-04T18:53:48.299942Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"(['A', 'B', 'C', 'D'], 4)"},"metadata":{}}]},{"cell_type":"code","source":"import json\nfrom tqdm import tqdm\nimport torch\nimport logging\nimport argparse\nimport numpy as np\n\ndef getResultdata(result_data):\n    choice_to_index = {letters[i]: i for i in range(len(letters))}\n    choice_to_index[None] = len(letters)\n\n    word_play = {}\n    reverse_play = {}\n    for item in result_data:\n        item_type = item['id'].split(\"-\")[0]\n        item_id = item['id'].split(\"-\")[1].split(\"_\")[0]\n        if item_type == 'WP':\n            if item_id not in word_play:\n                word_play[item_id] = [0,0,0]\n        else:\n            if item_id not in reverse_play:\n                reverse_play[item_id] = [0,0,0]\n\n    for item in result_data:\n        item_type = item['id'].split(\"-\")[0]\n        item_id = item['id'].split(\"-\")[1].split(\"_\")[0]\n        ad_type = 0\n        if 'SR' in item['id']:\n            ad_type = 1\n        elif 'CR' in item['id']:\n            ad_type = 2\n        else:\n            ad_type = 0\n\n        if item_type == 'WP':\n            if choice_to_index[item['predict']] == item['label']:\n                word_play[item_id][ad_type] = 1\n        else:\n            if choice_to_index[item['predict']] == item['label']:\n                reverse_play[item_id][ad_type] = 1\n                \n    return word_play,reverse_play\n\n\ndef getMetric(data_list):\n    data_list = np.array(data_list)\n    overall_accuracy = np.sum(data_list)/3/len(data_list)\n    original_accuracy = np.sum(data_list,axis = 0)[0]/len(data_list)\n    semantic_accuracy = np.sum(data_list,axis = 0)[1]/len(data_list)\n    context_accuracy = np.sum(data_list,axis = 0)[2]/len(data_list)\n    ori_sema = np.sum([1 if item[0]==1 and item[1] == 1 else 0 for item in data_list])/len(data_list)\n    ori_sema_cont = np.sum([1 if item[0]==1 and item[1] == 1 and item[2] == 1  else 0 for item in data_list])/len(data_list)\n    \n    print(\"over_all accuracy {}\".format(overall_accuracy))\n    print(\"single_original_accuracy {}\".format(original_accuracy))\n    print(\"single_semantic_accuracy {}\".format(semantic_accuracy))\n    print(\"single_context_accuracy {}\".format(context_accuracy))\n    print(\"sr_accuracy {}\".format(ori_sema))\n    print(\"cr_accuracy {}\".format(ori_sema_cont))\n\n    return {'over_all accuracy':overall_accuracy,'original_accuracy':original_accuracy,'semantic_accuracy':semantic_accuracy,'context_accuracy':context_accuracy,'ori_sema':ori_sema,'ori_sema_cont':ori_sema_cont}\n\n\ndef getSeperateResult(word_play,reverse_thinking):\n    final_result = {}\n    word_data_list = []\n    word_data_list = list(word_play.values())\n    print('#########Wordplay##########')\n    final_result['wordplay'] = getMetric(word_data_list)\n    \n    reverse_data_list = []\n    for item in reverse_thinking.values():\n        reverse_data_list.append(item)\n    print('#########Sentence##########')   \n    final_result['sentence'] = getMetric(reverse_data_list)  \n    \n    \n    all_data = word_data_list + reverse_data_list\n    print('#########All data##########') \n    final_result['all'] = getMetric(all_data) \n    \n    return final_result","metadata":{"_kg_hide-input":false,"execution":{"iopub.status.busy":"2024-06-04T18:53:48.303305Z","iopub.execute_input":"2024-06-04T18:53:48.303587Z","iopub.status.idle":"2024-06-04T18:53:48.372916Z","shell.execute_reply.started":"2024-06-04T18:53:48.303563Z","shell.execute_reply":"2024-06-04T18:53:48.371911Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"dirname= 'resultsnew'\npickle_dir = 'pickled_results'\n\ndef create_dir(name):\n    try:\n        os.mkdir(name)\n    except:\n        pass\n\ndef save(final_result, name=None, few_shot=False, example=format, data=None):\n    create_dir(dirname)\n    create_dir(pickle_dir)\n    \n    model_name = model_path.split('/')[-1]\n    inserted_name = \"\" if name is None else \"_\" + name\n    i = 1\n    while True:\n        file_path = f'{dirname}/{model_name}{inserted_name}_{i}.txt'\n        pickle_path = f'{pickle_dir}/{model_name}{inserted_name}_{i}.json'\n        try:\n            with open(file_path, 'r'):\n                i += 1\n        except:\n            with open(file_path, 'w') as file:\n                file.write(model_name+'\\n')\n                if few_shot:\n                    file.write(few_shot + '\\n')\n                if not isinstance(example, str):\n                    example = json.dumps(example)\n                file.write(example + '\\n')\n                file.write(json.dumps(final_result, indent=4))\n            if data is None:\n                print(f\"Warning, {model_name} results data not saving in json, please provide data arg\")\n            else:\n                with open(pickle_path, 'w') as file:\n                    file.write(json.dumps(data, indent=4))\n            return","metadata":{"execution":{"iopub.status.busy":"2024-06-04T18:53:48.374212Z","iopub.execute_input":"2024-06-04T18:53:48.374498Z","iopub.status.idle":"2024-06-04T18:53:48.389207Z","shell.execute_reply.started":"2024-06-04T18:53:48.374475Z","shell.execute_reply":"2024-06-04T18:53:48.388490Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"### Dataset\n\n- Skinuti s [linka](https://drive.google.com/drive/u/0/folders/1kiFXp5fqpf8--NQJJAlBIBpfSaXTk1UY)\n\n- Staviti u folder `/data`","metadata":{}},{"cell_type":"code","source":"device = 'cuda' if torch.cuda.is_available() else 'cpu'\ndevice","metadata":{"execution":{"iopub.status.busy":"2024-06-04T18:53:48.390279Z","iopub.execute_input":"2024-06-04T18:53:48.390529Z","iopub.status.idle":"2024-06-04T18:53:48.461601Z","shell.execute_reply.started":"2024-06-04T18:53:48.390507Z","shell.execute_reply":"2024-06-04T18:53:48.460583Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"'cuda'"},"metadata":{}}]},{"cell_type":"markdown","source":"### Load the model","metadata":{}},{"cell_type":"code","source":"from huggingface_hub import login\nlogin('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', add_to_git_credential=False, write_permission=True)","metadata":{"execution":{"iopub.status.busy":"2024-06-04T18:53:48.462964Z","iopub.execute_input":"2024-06-04T18:53:48.463791Z","iopub.status.idle":"2024-06-04T18:53:48.702658Z","shell.execute_reply.started":"2024-06-04T18:53:48.463755Z","shell.execute_reply":"2024-06-04T18:53:48.701733Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\nToken is valid (permission: write).\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\n","output_type":"stream"}]},{"cell_type":"code","source":"# model_path = 'mistralai/Mistral-7B-Instruct-v0.2'\n# model_path = 'microsoft/Phi-3-mini-128k-instruct'\n# model_path = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n# model_path = \"tiiuae/falcon-7b-instruct\"\nmodel_path = \"google/gemma-1.1-7b-it\"\n\nquantization_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_compute_dtype=torch.float16,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_use_double_quant=True,\n)\n\ndef get_model():\n    tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n    model = AutoModelForCausalLM.from_pretrained(\n        model_path,\n        device_map=\"auto\",\n        quantization_config=quantization_config,\n        trust_remote_code=True,\n    )\n    return tokenizer, model\n\ntokenizer, model = get_model()\n","metadata":{"execution":{"iopub.status.busy":"2024-06-04T18:53:48.703756Z","iopub.execute_input":"2024-06-04T18:53:48.704022Z","iopub.status.idle":"2024-06-04T18:56:36.835195Z","shell.execute_reply.started":"2024-06-04T18:53:48.703999Z","shell.execute_reply":"2024-06-04T18:56:36.834451Z"},"trusted":true},"execution_count":9,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/34.2k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7d02cb6a517f4c6ba7449c7d140aa562"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"20f0096867964cc99413cea4d22d9d6d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/17.5M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0c6542d6039842efbb4779ca00a08198"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/636 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e3d41771d30c4e45bd35318ab98b82c8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/620 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ad3b1498b7674c089169a246ee5359ef"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/20.9k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4c26c31453764d30b481458f6d30938d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9bc27f7a49b948fbbc758fb035ee909a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5c1ede7a66c04c32b6226b7ef542caf4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"454fb0b0154543e6ac206f185c9183e5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00003-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0e01925cc0304fccbb5b4f64f402015f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00004-of-00004.safetensors:   0%|          | 0.00/2.11G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bc7f5f1c87614631a0a2b94eaedfe2ff"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f7f7e79768fb47f5a6efe236244ca542"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/132 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4ee34c9339c94c64a9869217ca9d39b7"}},"metadata":{}}]},{"cell_type":"code","source":"data_path = '/kaggle/input/brainteaser/data'\nsentence_data_path = f\"{data_path}/SP-train.npy\"\nwordplay_data_list = f\"{data_path}/WP-train.npy\"\nsentence_data_list = list(np.load(sentence_data_path,allow_pickle=True))\nwordplay_data_list = list(np.load(wordplay_data_list,allow_pickle=True))\n\ntest_data_list = sentence_data_list + wordplay_data_list\nprint(f\"Dataset length {len(test_data_list)}\")","metadata":{"execution":{"iopub.status.busy":"2024-06-04T18:56:36.836597Z","iopub.execute_input":"2024-06-04T18:56:36.836953Z","iopub.status.idle":"2024-06-04T18:56:36.910817Z","shell.execute_reply.started":"2024-06-04T18:56:36.836919Z","shell.execute_reply":"2024-06-04T18:56:36.909984Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"Dataset length 903\n","output_type":"stream"}]},{"cell_type":"code","source":"format = \"\"\"The following question is a brainteaser.\n\nQuestion: {}\n\nChoices:\n(A) {}\n(B) {}\n(C) {}\n(D) {}\n\nAnswer:(\"\"\"","metadata":{"execution":{"iopub.status.busy":"2024-06-04T18:56:36.914633Z","iopub.execute_input":"2024-06-04T18:56:36.914887Z","iopub.status.idle":"2024-06-04T18:56:36.919069Z","shell.execute_reply.started":"2024-06-04T18:56:36.914866Z","shell.execute_reply":"2024-06-04T18:56:36.918230Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"def get_single_demo(sample):\n    sample_demo = format.format(\n        sample['question'], *sample['choice_list'])\n    return sample_demo","metadata":{"execution":{"iopub.status.busy":"2024-06-04T18:56:36.920119Z","iopub.execute_input":"2024-06-04T18:56:36.920403Z","iopub.status.idle":"2024-06-04T18:56:36.930534Z","shell.execute_reply.started":"2024-06-04T18:56:36.920379Z","shell.execute_reply":"2024-06-04T18:56:36.929812Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"print(get_single_demo(test_data_list[0]))","metadata":{"execution":{"iopub.status.busy":"2024-06-04T18:56:36.931578Z","iopub.execute_input":"2024-06-04T18:56:36.931938Z","iopub.status.idle":"2024-06-04T18:56:36.942714Z","shell.execute_reply.started":"2024-06-04T18:56:36.931906Z","shell.execute_reply":"2024-06-04T18:56:36.941769Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"The following question is a brainteaser.\n\nQuestion: Mr. and Mrs. Mustard have six daughters and each daughter has one brother. But there are only 9 people in the family, how is that possible?\n\nChoices:\n(A) Some daughters get married and have their own family.\n(B) Each daughter shares the same brother.\n(C) Some brothers were not loved by family and moved away.\n(D) None of above.\n\nAnswer:(\n","output_type":"stream"}]},{"cell_type":"code","source":"good_responses = [f'{x})' for x in letters]\ngood_responses","metadata":{"execution":{"iopub.status.busy":"2024-06-04T18:56:36.943684Z","iopub.execute_input":"2024-06-04T18:56:36.943953Z","iopub.status.idle":"2024-06-04T18:56:36.954912Z","shell.execute_reply.started":"2024-06-04T18:56:36.943922Z","shell.execute_reply":"2024-06-04T18:56:36.953954Z"},"trusted":true},"execution_count":14,"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"['A)', 'B)', 'C)', 'D)']"},"metadata":{}}]},{"cell_type":"code","source":"def set_predictions(data_list):\n    \"\"\"Uzima response i parsira ga tako da pod predict stavi slovo koje je napisano\"\"\"\n    for index,item in enumerate(data_list):\n        item['predict'] = None\n        for x in letters:\n            if (f'{x})') in item['response']:\n                item['predict'] = x\n\n        if item['predict'] is None:\n            print(index)\n\ndef custom_stopping_criteria(input_ids: torch.LongTensor, score: torch.FloatTensor, **kwargs) -> bool:\n    \"\"\"\n    Funkcija kako bi se dinamicki prepoznalo kad treba prestati generirati tekst:\n    ne nakon fiksnog broja tokena, vec kad model napise rjesenje. Najcesce\n    ce to ipak biti medu prva dva tokena\n    \"\"\"\n    decoded = tokenizer.decode(input_ids[0][-3:])\n    for good_response in good_responses:\n        if good_response in decoded:\n            return True\n    return False\n\nstopping_criteria = StoppingCriteriaList([custom_stopping_criteria])","metadata":{"execution":{"iopub.status.busy":"2024-06-04T18:56:36.955948Z","iopub.execute_input":"2024-06-04T18:56:36.956316Z","iopub.status.idle":"2024-06-04T18:56:36.965123Z","shell.execute_reply.started":"2024-06-04T18:56:36.956291Z","shell.execute_reply":"2024-06-04T18:56:36.964437Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"def generate(samples, tokens=10, all_tokens=True, few_shot=False):\n    global model\n    for sample in tqdm(samples):\n        if few_shot:\n            text = demonstration + get_single_demo(sample)\n        else:\n            text = get_single_demo(sample)\n        inputs = tokenizer.encode(text, return_tensors=\"pt\", return_attention_mask=False).to(device)\n        original_tokens = len(inputs[0])\n        outputs = model.generate(\n            inputs,\n            pad_token_id=tokenizer.eos_token_id,\n            do_sample = False,\n            max_new_tokens=tokens,\n            stopping_criteria=stopping_criteria\n        )\n        outputs = outputs[0][0 if all_tokens else original_tokens:]\n        sample['response'] = tokenizer.decode(outputs)\n    set_predictions(samples)","metadata":{"execution":{"iopub.status.busy":"2024-06-04T18:56:36.966207Z","iopub.execute_input":"2024-06-04T18:56:36.966465Z","iopub.status.idle":"2024-06-04T18:56:36.982972Z","shell.execute_reply.started":"2024-06-04T18:56:36.966443Z","shell.execute_reply":"2024-06-04T18:56:36.982234Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"# def all_same_incorrect_answers(i):\n#     current = test_data_list[i]\n#     new = {**current}\n#     choice_list = [*current['choice_list']]\n#     correct_answer = choice_list[current[\"label\"]]\n#     choice_list.remove(correct_answer)\n#     incorrect_answer = choice_list[0]\n#     choice_list.clear()\n#     choice_list.insert(0,correct_answer)\n#     choice_list.insert(1,incorrect_answer)\n#     choice_list.insert(2,incorrect_answer)\n#     choice_list.insert(3,incorrect_answer)\n#     random.shuffle(choice_list)\n    \n#     new['choice_list'] = choice_list\n#     new['label'] = choice_list.index(correct_answer)\n#     return new\n\n# generated_data_list = []\n# for i in range(len(test_data_list)):\n#     new = all_same_incorrect_answers(i)\n#     generated_data_list.append(new)\n    \n# print(generated_data_list[0])\n# print(generated_data_list[100])\n# print(generated_data_list[500])\n\n# # Save the list to a file\n# import pickle\n# with open('generated_data_list.pkl', 'wb') as file:\n#     pickle.dump(generated_data_list, file)","metadata":{"execution":{"iopub.status.busy":"2024-06-04T18:56:36.983927Z","iopub.execute_input":"2024-06-04T18:56:36.984251Z","iopub.status.idle":"2024-06-04T18:56:36.995536Z","shell.execute_reply.started":"2024-06-04T18:56:36.984226Z","shell.execute_reply":"2024-06-04T18:56:36.994710Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"# Učitavanje generirane liste da za sve modele bude ista (jer imamo random.shuffle) - za always A/B/C/D\nimport pickle\nwith open('/kaggle/input/generated-data-list/generated_data_list.pkl', 'rb') as file:\n    generated_data_list = pickle.load(file)","metadata":{"execution":{"iopub.status.busy":"2024-06-04T18:56:36.996697Z","iopub.execute_input":"2024-06-04T18:56:36.997356Z","iopub.status.idle":"2024-06-04T18:56:37.031960Z","shell.execute_reply.started":"2024-06-04T18:56:36.997325Z","shell.execute_reply":"2024-06-04T18:56:37.031052Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"print(generated_data_list[0])","metadata":{"execution":{"iopub.status.busy":"2024-06-04T18:56:37.033022Z","iopub.execute_input":"2024-06-04T18:56:37.033333Z","iopub.status.idle":"2024-06-04T18:56:37.038008Z","shell.execute_reply.started":"2024-06-04T18:56:37.033309Z","shell.execute_reply":"2024-06-04T18:56:37.037048Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"{'id': 'SP-0', 'question': 'Mr. and Mrs. Mustard have six daughters and each daughter has one brother. But there are only 9 people in the family, how is that possible?', 'answer': 'Each daughter shares the same brother.', 'distractor1': 'Some daughters get married and have their own family.', 'distractor2': 'Some brothers were not loved by family and moved away.', 'distractor(unsure)': 'None of above.', 'label': 0, 'choice_list': ['Each daughter shares the same brother.', 'Some daughters get married and have their own family.', 'Some daughters get married and have their own family.', 'Some daughters get married and have their own family.'], 'choice_order': [1, 0, 2, 3], 'response': 'B)', 'predict': 'B'}\n","output_type":"stream"}]},{"cell_type":"code","source":"generate(generated_data_list[:10], tokens=1000, all_tokens=False, few_shot=False)","metadata":{"execution":{"iopub.status.busy":"2024-06-04T18:56:37.039266Z","iopub.execute_input":"2024-06-04T18:56:37.039533Z","iopub.status.idle":"2024-06-04T18:56:42.326298Z","shell.execute_reply.started":"2024-06-04T18:56:37.039511Z","shell.execute_reply":"2024-06-04T18:56:42.325330Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stderr","text":"100%|██████████| 10/10 [00:05<00:00,  1.90it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"for sample in generated_data_list[:5]:\n    print(\"\\n>\", sample['response'])","metadata":{"execution":{"iopub.status.busy":"2024-06-04T18:56:42.327561Z","iopub.execute_input":"2024-06-04T18:56:42.328305Z","iopub.status.idle":"2024-06-04T18:56:42.335769Z","shell.execute_reply.started":"2024-06-04T18:56:42.328271Z","shell.execute_reply":"2024-06-04T18:56:42.334400Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"\n> C)\n\n> B)\n\n> C)\n\n> C)\n\n> B)\n","output_type":"stream"}]},{"cell_type":"code","source":"generate(generated_data_list, tokens=20, all_tokens=False, few_shot=False)","metadata":{"execution":{"iopub.status.busy":"2024-06-04T18:56:42.337256Z","iopub.execute_input":"2024-06-04T18:56:42.337633Z","iopub.status.idle":"2024-06-04T19:02:08.954722Z","shell.execute_reply.started":"2024-06-04T18:56:42.337598Z","shell.execute_reply":"2024-06-04T19:02:08.953738Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stderr","text":"100%|██████████| 903/903 [05:26<00:00,  2.76it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"word_play,sentence_play = getResultdata(generated_data_list)\nfinal_result = getSeperateResult(word_play, sentence_play)","metadata":{"execution":{"iopub.status.busy":"2024-06-04T19:02:08.955820Z","iopub.execute_input":"2024-06-04T19:02:08.956081Z","iopub.status.idle":"2024-06-04T19:02:08.965883Z","shell.execute_reply.started":"2024-06-04T19:02:08.956059Z","shell.execute_reply":"2024-06-04T19:02:08.964899Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"#########Wordplay##########\nover_all accuracy 0.4671717171717171\nsingle_original_accuracy 0.48484848484848486\nsingle_semantic_accuracy 0.4090909090909091\nsingle_context_accuracy 0.5075757575757576\nsr_accuracy 0.3181818181818182\ncr_accuracy 0.21212121212121213\n#########Sentence##########\nover_all accuracy 0.621301775147929\nsingle_original_accuracy 0.6390532544378699\nsingle_semantic_accuracy 0.591715976331361\nsingle_context_accuracy 0.6331360946745562\nsr_accuracy 0.48520710059171596\ncr_accuracy 0.35502958579881655\n#########All data##########\nover_all accuracy 0.5537098560354374\nsingle_original_accuracy 0.5714285714285714\nsingle_semantic_accuracy 0.5116279069767442\nsingle_context_accuracy 0.5780730897009967\nsr_accuracy 0.4119601328903654\ncr_accuracy 0.292358803986711\n","output_type":"stream"}]},{"cell_type":"code","source":"# save(final_result, name=\"same_incorrect_answers\", example=get_single_demo(generated_data_list[0]))\nsave(final_result, name=\"all_incorrect_same\", example=get_single_demo(generated_data_list[0]), data=generated_data_list)","metadata":{"execution":{"iopub.status.busy":"2024-06-04T19:02:08.967255Z","iopub.execute_input":"2024-06-04T19:02:08.967543Z","iopub.status.idle":"2024-06-04T19:02:09.011128Z","shell.execute_reply.started":"2024-06-04T19:02:08.967518Z","shell.execute_reply":"2024-06-04T19:02:09.010136Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"# Ciscenje prije loadanja novog modela\nimport gc\ndel model\nmodel = None\ngc.collect()\ntorch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2024-06-04T19:02:09.012561Z","iopub.execute_input":"2024-06-04T19:02:09.013373Z","iopub.status.idle":"2024-06-04T19:02:09.584205Z","shell.execute_reply.started":"2024-06-04T19:02:09.013333Z","shell.execute_reply":"2024-06-04T19:02:09.583020Z"},"trusted":true},"execution_count":25,"outputs":[]}]}