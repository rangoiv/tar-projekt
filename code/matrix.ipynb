{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Brain teaser\n","\n","[Original Github REPO](https://github.com/1171-jpg/BrainTeaser)\n","\n","[Leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["!pip install -q -U langchain transformers bitsandbytes accelerate"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import numpy as np\n","import json\n","from datasets import load_dataset\n","from random import shuffle\n","import random\n","\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import seaborn as sns\n","\n","import torch\n","from torch.nn.functional import normalize\n","from transformers import AutoModel, AutoTokenizer\n","\n","from tqdm.auto import tqdm\n","import numpy as np\n","import os\n","from transformers import StoppingCriteriaList\n","\n","from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline\n","import torch\n","from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, AutoModelForCausalLM\n","import gc"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["good_responses, letters = None, None\n","def set_letters(k):\n","    global letters, good_responses\n","    letters = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P'][:k]\n","    good_responses = [f'{x})' for x in letters]\n","set_letters(4)\n","print(letters)\n","print(good_responses)"]},{"cell_type":"markdown","metadata":{},"source":["## Metrike\n","\n","Preuzeto direktno s njihovog repoa"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":false,"trusted":true},"outputs":[],"source":["import json\n","from tqdm import tqdm\n","import torch\n","import logging\n","import argparse\n","import numpy as np\n","\n","def getResultdata(result_data):\n","    global letters\n","    choice_to_index = {letters[i]: i for i in range(len(letters))}\n","    choice_to_index[None] = len(letters)\n","\n","    word_play = {}\n","    reverse_play = {}\n","    for item in result_data:\n","        item_type = item['id'].split(\"-\")[0]\n","        item_id = item['id'].split(\"-\")[1].split(\"_\")[0]\n","        if item_type == 'WP':\n","            if item_id not in word_play:\n","                word_play[item_id] = [0,0,0]\n","        else:\n","            if item_id not in reverse_play:\n","                reverse_play[item_id] = [0,0,0]\n","\n","    for item in result_data:\n","        item_type = item['id'].split(\"-\")[0]\n","        item_id = item['id'].split(\"-\")[1].split(\"_\")[0]\n","        ad_type = 0\n","        if 'SR' in item['id']:\n","            ad_type = 1\n","        elif 'CR' in item['id']:\n","            ad_type = 2\n","        else:\n","            ad_type = 0\n","\n","        if item_type == 'WP':\n","            if choice_to_index[item['predict']] == item['label']:\n","                word_play[item_id][ad_type] = 1\n","        else:\n","            if choice_to_index[item['predict']] == item['label']:\n","                reverse_play[item_id][ad_type] = 1\n","                \n","    return word_play,reverse_play\n","\n","\n","def getMetric(data_list):\n","    data_list = np.array(data_list)\n","    overall_accuracy = np.sum(data_list)/3/len(data_list)\n","    original_accuracy = np.sum(data_list,axis = 0)[0]/len(data_list)\n","    semantic_accuracy = np.sum(data_list,axis = 0)[1]/len(data_list)\n","    context_accuracy = np.sum(data_list,axis = 0)[2]/len(data_list)\n","    ori_sema = np.sum([1 if item[0]==1 and item[1] == 1 else 0 for item in data_list])/len(data_list)\n","    ori_sema_cont = np.sum([1 if item[0]==1 and item[1] == 1 and item[2] == 1  else 0 for item in data_list])/len(data_list)\n","    \n","    print(\"over_all accuracy {}\".format(overall_accuracy))\n","    print(\"single_original_accuracy {}\".format(original_accuracy))\n","    print(\"single_semantic_accuracy {}\".format(semantic_accuracy))\n","    print(\"single_context_accuracy {}\".format(context_accuracy))\n","    print(\"sr_accuracy {}\".format(ori_sema))\n","    print(\"cr_accuracy {}\".format(ori_sema_cont))\n","\n","    return {'over_all accuracy':overall_accuracy,'original_accuracy':original_accuracy,'semantic_accuracy':semantic_accuracy,'context_accuracy':context_accuracy,'ori_sema':ori_sema,'ori_sema_cont':ori_sema_cont}\n","\n","\n","def getSeperateResult(word_play,reverse_thinking):\n","    final_result = {}\n","    word_data_list = []\n","    word_data_list = list(word_play.values())\n","    print('#########Wordplay##########')\n","    final_result['wordplay'] = getMetric(word_data_list)\n","    \n","    reverse_data_list = []\n","    for item in reverse_thinking.values():\n","        reverse_data_list.append(item)\n","    print('#########Sentence##########')   \n","    final_result['sentence'] = getMetric(reverse_data_list)  \n","    \n","    \n","    all_data = word_data_list + reverse_data_list\n","    print('#########All data##########') \n","    final_result['all'] = getMetric(all_data) \n","    \n","    return final_result"]},{"cell_type":"markdown","metadata":{},"source":["### Spremanje datoteka\n","\n","Automatski dodaje +1 na naziv ako vec postoji takva datoteka\n","\n","Uzima format model_name + name + redni broj za datoteku\n","\n","Unutra pise prvi testni primjer (example) i rezultate"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["dirname = 'resultsnew'\n","pickle_dir = 'pickled_results'\n","\n","def create_dir(name):\n","    \"\"\"Create a directory if it doesn't exist.\"\"\"\n","    try:\n","        os.mkdir(name)\n","    except FileExistsError:\n","        pass\n","\n","def tensor_to_serializable(obj):\n","    \"\"\"\n","    Helper function to convert non-serializable objects like PyTorch tensors \n","    into a JSON serializable format.\n","    \"\"\"\n","    if isinstance(obj, torch.Tensor):\n","        return obj.tolist()  # Convert tensors to lists\n","    raise TypeError(f\"Object of type {obj.__class__.__name__} is not JSON serializable\")\n","\n","def save(final_result, name=None, few_shot=False, example=None, data=None):\n","    \"\"\"Save the final result and data into text and JSON files.\"\"\"\n","    create_dir(dirname)\n","    create_dir(pickle_dir)\n","    \n","    model_name = model_path.split('/')[-1]\n","    inserted_name = \"\" if name is None else \"_\" + name\n","    i = 1\n","    \n","    while True:\n","        file_path = f'{dirname}/{model_name}{inserted_name}_{i}.txt'\n","        pickle_path = f'{pickle_dir}/{model_name}{inserted_name}_{i}.json'\n","        try:\n","            with open(file_path, 'r'):\n","                i += 1\n","        except FileNotFoundError:\n","            # Writing the results to a text file\n","            with open(file_path, 'w') as file:\n","                file.write(model_name + '\\n')\n","                if few_shot:\n","                    file.write(few_shot + '\\n')\n","                if example:\n","                    if not isinstance(example, str):\n","                        example = json.dumps(example, indent=4)\n","                    file.write(example + '\\n')\n","                file.write(json.dumps(final_result, indent=4, default=tensor_to_serializable))\n","            \n","            # Writing data to a JSON file\n","            if data is None:\n","                print(f\"Warning: {model_name} results data not saving in json, please provide data arg\")\n","            else:\n","                with open(pickle_path, 'w') as file:\n","                    file.write(json.dumps(data, indent=4, default=tensor_to_serializable))\n","            \n","            return\n"]},{"cell_type":"markdown","metadata":{},"source":["### Load the model\n","\n","Login kako bi se mogli ucitati napredniji modeli"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","device"]},{"cell_type":"markdown","metadata":{},"source":["Logiranje u huggingiface\n","treba kljuc, njega nabavite na huggingface stranici"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from huggingface_hub import login\n","login('', add_to_git_credential=False, write_permission=True)"]},{"cell_type":"markdown","metadata":{},"source":["BitsAndBytesConfig je novi, to kao kvantizira model pa stane na graficku"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# model_path = 'google/flan-t5-xxl'\n","# model_path = 'google/flan-t5-xl'\n","# model_path = 'google/flan-t5-large'\n","# model_path = 'mistralai/Mistral-7B-Instruct-v0.2'\n","# model_path = 'mistralai/Mistral-7B-v0.3'\n","# model_path = 'microsoft/phi-2'\n","# model_path = 'microsoft/Phi-3-mini-128k-instruct'\n","# model_path = 'microsoft/Phi-3-mini-4k-instruct'\n","# model_path = '01-ai/Yi-1.5-6B'\n","# model_path = 'netcat420/MFANN3bv0.6'\n","model_path = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n","# model_path = \"meta-llama/Meta-Llama-3-8B\"\n","# model_path = \"allenai/OLMo-7B-Instruct\"\n","# model_path = \"tiiuae/falcon-7b-instruct\"\n","# model_path = \"tiiuae/falcon-7b\"\n","# model_path = 'Intel/neural-chat-7b-v3-1'\n","# model_path = \"google/gemma-1.1-7b-it\"\n","\n","quantization_config = BitsAndBytesConfig(\n","    load_in_4bit=True,\n","    bnb_4bit_compute_dtype=torch.float16,\n","    bnb_4bit_quant_type=\"nf4\",\n","    bnb_4bit_use_double_quant=True,\n",")\n","\n","def get_mistral():\n","    tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n","    model = AutoModelForCausalLM.from_pretrained(\n","        model_path,\n","        device_map=\"auto\",\n","        quantization_config=quantization_config,\n","        trust_remote_code=True,\n","    )\n","    return tokenizer, model\n","\n","def get_llama():\n","    tokenizer = AutoTokenizer.from_pretrained(model_path)\n","    model_4bit = AutoModelForCausalLM.from_pretrained(\n","        model_path,\n","#         \"SweatyCrayfish/llama-3-8b-quantized\", \n","#         load_in_4bit=True, \n","#         torch_dtype=torch.float16,\n","        device_map=\"auto\",\n","        quantization_config=quantization_config,\n","    )\n","    return tokenizer, model_4bit\n","\n","tokenizer, model = get_llama()"]},{"cell_type":"markdown","metadata":{},"source":["### Dataset\n","\n","- Skinuti s [linka](https://drive.google.com/drive/u/0/folders/1kiFXp5fqpf8--NQJJAlBIBpfSaXTk1UY)\n","\n","- Staviti u folder `/data`, ako na Kaggleu, onda napraviti svoj dataset te ga dodati pod input"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# few_shot_examples = list(np.load(\"./{dataset_path}/data/demonstration.npy\",allow_pickle=True))\n","data_path = '/kaggle/input/brainteaser/data'\n","sentence_data_path = f\"{data_path}/SP-train.npy\"\n","wordplay_data_list = f\"{data_path}/WP-train.npy\"\n","sentence_data_list = list(np.load(sentence_data_path,allow_pickle=True))\n","wordplay_data_list = list(np.load(wordplay_data_list,allow_pickle=True))\n","\n","test_data_list = sentence_data_list + wordplay_data_list\n","print(f\"Dataset length {len(test_data_list)}\")"]},{"cell_type":"markdown","metadata":{},"source":["Format za mystral, za ostale modele maknuti linije `<s>[INST]`"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["format = \"\"\n","def get_format():\n","    global format, letters\n","    newline = '\\n'\n","    format = f\"\"\"Question: {\"{}\"}\n","\n","Choices:\n","{''.join('(' + x + ') {}' + newline for x in letters)}\"\"\"\n","    return format\n","\n","def get_llama_format(demonstration=None):\n","    global tokenizer, format\n","    messages = []\n","    if demonstration is not None:\n","        messages.append({\"role\": \"system\", \"content\": demonstration})\n","    messages.append({\"role\": \"user\", \"content\": get_format()})\n","    messages.append({\"role\": \"assistant\", \"content\": \"Answer:(\"})\n","    try:\n","        format = tokenizer.apply_chat_template(messages, tokenize=False)\n","    except Exception:\n","        # When using mistral model, roles in messages must alternate, this way it works without errors\n","        messages[1][\"content\"] = demonstration + \"\\n\\n\" + messages[1][\"content\"]\n","        format = tokenizer.apply_chat_template(messages[1:], tokenize=False)\n","    # remove \"<|eot_id|>\" from the end\n","    format = format[:format.index(\"Answer:(\")+8]\n","\n","def get_appended_format(demonstration=None):\n","    global format\n","    format = get_format()\n","    if demonstration is not None:\n","        format = demonstration + \"\\n\\n\" + format\n","    format += \"\\nAnswer:(\"\n","\n","# get_llama_format(\"Primjer\")\n","set_letters(4)\n","get_appended_format()\n","print(format)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def get_single_demo(sample):\n","    global format\n","    sample_demo = format.format(\n","        sample['question'], *sample['choice_list'])\n","    return sample_demo"]},{"cell_type":"markdown","metadata":{},"source":["## Generiranje dataseta\n","\n","### Pomocne funkcije"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def set_predictions(data_list):\n","    \"\"\"Uzima response i parsira ga tako da pod predict stavi slovo koje je napisano\"\"\"\n","    global letters\n","    for index,item in enumerate(data_list):\n","        item['predict'] = None\n","        for x in letters:\n","            if (f'{x})') in item['response']:\n","                item['predict'] = x\n","\n","        if item['predict'] is None:\n","            print(index)\n","\n","def custom_stopping_criteria(input_ids: torch.LongTensor, score: torch.FloatTensor, **kwargs) -> bool:\n","    \"\"\"\n","    Funkcija kako bi se dinamicki prepoznalo kad treba prestati generirati tekst:\n","    ne nakon fiksnog broja tokena, vec kad model napise rjesenje. Najcesce\n","    ce to ipak biti medu prva dva tokena\n","    \"\"\"\n","    global good_responses\n","    decoded = tokenizer.decode(input_ids[0][-3:])\n","    for good_response in good_responses:\n","        if good_response in decoded:\n","            return True\n","    return False\n","\n","stopping_criteria = StoppingCriteriaList([custom_stopping_criteria])"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def generate(samples, tokens=10, all_tokens=True, few_shot=False):\n","    global model\n","    for sample in tqdm(samples):\n","        # Prepare the input text\n","        if few_shot:\n","            text = demonstration + get_single_demo(sample)\n","        else:\n","            text = get_single_demo(sample)\n","        \n","        # Encode the input using the tokenizer\n","        inputs = tokenizer.encode(text, return_tensors=\"pt\", return_attention_mask=False).to(device)\n","        \n","        # Generate the output text\n","        with torch.no_grad():\n","            results = model(inputs)\n","            # Get the logits and calculate the softmax probabilities for the last token\n","            logits = results.logits.squeeze()[-1]\n","            probs = torch.nn.functional.softmax(logits, dim=-1)  # Apply softmax to get probabilities\n","            sample['probs'] = probs  # Store probabilities in the sample\n","\n","        # Generate the final response (text)\n","        outputs = model.generate(\n","            inputs,\n","            pad_token_id=tokenizer.eos_token_id,\n","            do_sample=False,\n","            max_new_tokens=tokens,\n","            stopping_criteria=stopping_criteria\n","        )\n","        \n","        # Extract the generated output\n","        original_tokens = len(inputs[0])\n","        generated_output = outputs[0][0 if all_tokens else original_tokens:]\n","        response = tokenizer.decode(generated_output, skip_special_tokens=True)\n","        \n","        # Store the response in the sample\n","        sample['response'] = response\n","    \n","    # Process the predictions (e.g., based on the model outputs)\n","    set_predictions(samples)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["print(test_data_list[0])"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def turn_to_gibberish():\n","    output_name = \"gibberish_all\"\n","    \n","    random.seed(10)\n","    for dict in test_data_list:\n","        choice_list = dict[\"choice_list\"]\n","        for i in range(len(choice_list)):\n","            if i != dict[\"label\"]:\n","                s = choice_list[i]\n","                choice_list[i] = ''.join(random.sample(s,len(s)))\n","        \n","        if dict[\"id\"] == \"SP-88\":\n","            for key,item in dict.items():\n","                print(f\"{key}: {item}\")\n","    \n","    return test_data_list, output_name\n","generated_data_list, output_name = turn_to_gibberish()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["generate(test_data_list, tokens=20, all_tokens=False, few_shot=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def get_probs(data, dict_matrix, threshold):\n","    letters = ['a', 'A', 'b', 'B', 'c', 'C', 'd', 'D']\n","    total_probs = {}  # Dictionary to store the cumulative probabilities for each letter\n","    count = 0\n","    total_prob = 0\n","    \n","    probs = data['probs']  # Assuming this contains probabilities from the model\n","    cond = probs > probs.max() * 0.0001  # Filter out very low probabilities\n","    indices = cond.nonzero()  # Get indices of significant probabilities\n","    values = probs[cond]  # Get the values of those probabilities\n","    \n","    # Sum the probabilities for each relevant letter\n","    for ind, val in zip(indices[:, 0], values):\n","        letter = tokenizer.decode(ind)\n","        if letter not in total_probs:\n","            total_probs[letter] = 0  # Initialize the letter if not already present\n","        total_probs[letter] += float(val)  # Add the probability for the letter\n","        total_prob += val  # Sum up the total probabilities\n","    \n","    # Normalize total_probs by dividing each by total_prob\n","    if float(total_prob) > 0:  # Prevent division by zero\n","        for letter in total_probs:\n","            total_probs[letter] /= total_prob  # Normalize each probability\n","            \n","    sorted_total_probs = sorted(total_probs.items(), key=lambda x: x[1], reverse=True)\n","    letters = ['A', 'B', 'C', 'D']\n","    choice_to_index = {letters[i]: i for i in range(len(letters))}\n","    choice_to_index[None] = len(letters)\n","    if choice_to_index[data['predict']] == data['label']:\n","        let, prob = sorted_total_probs[0]\n","        if prob >= threshold:\n","            dict_matrix['correct_high'].append(1)\n","        else:\n","            dict_matrix['correct_low'].append(1)\n","    else:\n","        let, prob = sorted_total_probs[0]\n","        if prob > threshold:\n","            dict_matrix['incorrect_high'].append(1)\n","        else:\n","            dict_matrix['incorrect_low'].append(1)\n","    return"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["dict_matrix = {}\n","dict_matrix['correct_high'] = []\n","dict_matrix['correct_low'] = []\n","dict_matrix['incorrect_high'] = []\n","dict_matrix['incorrect_low'] = []\n","\n","for item in test_data_list:\n","    get_probs(item, dict_matrix, 0.5)\n","suma = 0\n","for k, v in dict_matrix.items():\n","    suma += sum(v)\n","for k, v in dict_matrix.items():\n","    dict_matrix[k] = round(sum(v) / suma, 3)\n","print(dict_matrix)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["word_play, sentence_play = getResultdata(test_data_list)\n","final_result = getSeperateResult(word_play, sentence_play)\n","final_result.update(dict_matrix)  # This merges dict_matrix into final_result\n","save(final_result, name=\"original\", example=get_single_demo(test_data_list[0]), data=test_data_list)"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":4992831,"sourceId":8393115,"sourceType":"datasetVersion"}],"dockerImageVersionId":30787,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"}},"nbformat":4,"nbformat_minor":4}
