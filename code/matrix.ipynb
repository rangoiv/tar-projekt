{"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8393115,"sourceType":"datasetVersion","datasetId":4992831}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Brain teaser\n\n[Original Github REPO](https://github.com/1171-jpg/BrainTeaser)\n\n[Leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard)\n","metadata":{}},{"cell_type":"code","source":"!pip install -q -U langchain transformers bitsandbytes accelerate","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport json\nfrom datasets import load_dataset\nfrom random import shuffle\nimport random\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\n\nimport torch\nfrom torch.nn.functional import normalize\nfrom transformers import AutoModel, AutoTokenizer\n\nfrom tqdm.auto import tqdm\nimport numpy as np\nimport os\nfrom transformers import StoppingCriteriaList\n\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM, AutoModelForCausalLM\nimport gc","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"good_responses, letters = None, None\ndef set_letters(k):\n    global letters, good_responses\n    letters = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P'][:k]\n    good_responses = [f'{x})' for x in letters]\nset_letters(4)\nprint(letters)\nprint(good_responses)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Metrike\n\nPreuzeto direktno s njihovog repoa","metadata":{}},{"cell_type":"code","source":"import json\nfrom tqdm import tqdm\nimport torch\nimport logging\nimport argparse\nimport numpy as np\n\ndef getResultdata(result_data):\n    global letters\n    choice_to_index = {letters[i]: i for i in range(len(letters))}\n    choice_to_index[None] = len(letters)\n\n    word_play = {}\n    reverse_play = {}\n    for item in result_data:\n        item_type = item['id'].split(\"-\")[0]\n        item_id = item['id'].split(\"-\")[1].split(\"_\")[0]\n        if item_type == 'WP':\n            if item_id not in word_play:\n                word_play[item_id] = [0,0,0]\n        else:\n            if item_id not in reverse_play:\n                reverse_play[item_id] = [0,0,0]\n\n    for item in result_data:\n        item_type = item['id'].split(\"-\")[0]\n        item_id = item['id'].split(\"-\")[1].split(\"_\")[0]\n        ad_type = 0\n        if 'SR' in item['id']:\n            ad_type = 1\n        elif 'CR' in item['id']:\n            ad_type = 2\n        else:\n            ad_type = 0\n\n        if item_type == 'WP':\n            if choice_to_index[item['predict']] == item['label']:\n                word_play[item_id][ad_type] = 1\n        else:\n            if choice_to_index[item['predict']] == item['label']:\n                reverse_play[item_id][ad_type] = 1\n                \n    return word_play,reverse_play\n\n\ndef getMetric(data_list):\n    data_list = np.array(data_list)\n    overall_accuracy = np.sum(data_list)/3/len(data_list)\n    original_accuracy = np.sum(data_list,axis = 0)[0]/len(data_list)\n    semantic_accuracy = np.sum(data_list,axis = 0)[1]/len(data_list)\n    context_accuracy = np.sum(data_list,axis = 0)[2]/len(data_list)\n    ori_sema = np.sum([1 if item[0]==1 and item[1] == 1 else 0 for item in data_list])/len(data_list)\n    ori_sema_cont = np.sum([1 if item[0]==1 and item[1] == 1 and item[2] == 1  else 0 for item in data_list])/len(data_list)\n    \n    print(\"over_all accuracy {}\".format(overall_accuracy))\n    print(\"single_original_accuracy {}\".format(original_accuracy))\n    print(\"single_semantic_accuracy {}\".format(semantic_accuracy))\n    print(\"single_context_accuracy {}\".format(context_accuracy))\n    print(\"sr_accuracy {}\".format(ori_sema))\n    print(\"cr_accuracy {}\".format(ori_sema_cont))\n\n    return {'over_all accuracy':overall_accuracy,'original_accuracy':original_accuracy,'semantic_accuracy':semantic_accuracy,'context_accuracy':context_accuracy,'ori_sema':ori_sema,'ori_sema_cont':ori_sema_cont}\n\n\ndef getSeperateResult(word_play,reverse_thinking):\n    final_result = {}\n    word_data_list = []\n    word_data_list = list(word_play.values())\n    print('#########Wordplay##########')\n    final_result['wordplay'] = getMetric(word_data_list)\n    \n    reverse_data_list = []\n    for item in reverse_thinking.values():\n        reverse_data_list.append(item)\n    print('#########Sentence##########')   \n    final_result['sentence'] = getMetric(reverse_data_list)  \n    \n    \n    all_data = word_data_list + reverse_data_list\n    print('#########All data##########') \n    final_result['all'] = getMetric(all_data) \n    \n    return final_result","metadata":{"_kg_hide-input":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Spremanje datoteka\n\nAutomatski dodaje +1 na naziv ako vec postoji takva datoteka\n\nUzima format model_name + name + redni broj za datoteku\n\nUnutra pise prvi testni primjer (example) i rezultate","metadata":{}},{"cell_type":"code","source":"dirname = 'resultsnew'\npickle_dir = 'pickled_results'\n\ndef create_dir(name):\n    \"\"\"Create a directory if it doesn't exist.\"\"\"\n    try:\n        os.mkdir(name)\n    except FileExistsError:\n        pass\n\ndef tensor_to_serializable(obj):\n    \"\"\"\n    Helper function to convert non-serializable objects like PyTorch tensors \n    into a JSON serializable format.\n    \"\"\"\n    if isinstance(obj, torch.Tensor):\n        return obj.tolist()  # Convert tensors to lists\n    raise TypeError(f\"Object of type {obj.__class__.__name__} is not JSON serializable\")\n\ndef save(final_result, name=None, few_shot=False, example=None, data=None):\n    \"\"\"Save the final result and data into text and JSON files.\"\"\"\n    create_dir(dirname)\n    create_dir(pickle_dir)\n    \n    model_name = model_path.split('/')[-1]\n    inserted_name = \"\" if name is None else \"_\" + name\n    i = 1\n    \n    while True:\n        file_path = f'{dirname}/{model_name}{inserted_name}_{i}.txt'\n        pickle_path = f'{pickle_dir}/{model_name}{inserted_name}_{i}.json'\n        try:\n            with open(file_path, 'r'):\n                i += 1\n        except FileNotFoundError:\n            # Writing the results to a text file\n            with open(file_path, 'w') as file:\n                file.write(model_name + '\\n')\n                if few_shot:\n                    file.write(few_shot + '\\n')\n                if example:\n                    if not isinstance(example, str):\n                        example = json.dumps(example, indent=4)\n                    file.write(example + '\\n')\n                file.write(json.dumps(final_result, indent=4, default=tensor_to_serializable))\n            \n            # Writing data to a JSON file\n            if data is None:\n                print(f\"Warning: {model_name} results data not saving in json, please provide data arg\")\n            else:\n                with open(pickle_path, 'w') as file:\n                    file.write(json.dumps(data, indent=4, default=tensor_to_serializable))\n            \n            return\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Load the model\n\nLogin kako bi se mogli ucitati napredniji modeli","metadata":{}},{"cell_type":"code","source":"device = 'cuda' if torch.cuda.is_available() else 'cpu'\ndevice","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Logiranje u huggingiface\ntreba kljuc, njega nabavite na huggingface stranici","metadata":{}},{"cell_type":"code","source":"from huggingface_hub import login\nlogin('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', add_to_git_credential=False, write_permission=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"BitsAndBytesConfig je novi, to kao kvantizira model pa stane na graficku","metadata":{}},{"cell_type":"code","source":"# model_path = 'google/flan-t5-xxl'\n# model_path = 'google/flan-t5-xl'\n# model_path = 'google/flan-t5-large'\n# model_path = 'mistralai/Mistral-7B-Instruct-v0.2'\n# model_path = 'mistralai/Mistral-7B-v0.3'\n# model_path = 'microsoft/phi-2'\n# model_path = 'microsoft/Phi-3-mini-128k-instruct'\n# model_path = 'microsoft/Phi-3-mini-4k-instruct'\n# model_path = '01-ai/Yi-1.5-6B'\n# model_path = 'netcat420/MFANN3bv0.6'\nmodel_path = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n# model_path = \"meta-llama/Meta-Llama-3-8B\"\n# model_path = \"allenai/OLMo-7B-Instruct\"\n# model_path = \"tiiuae/falcon-7b-instruct\"\n# model_path = \"tiiuae/falcon-7b\"\n# model_path = 'Intel/neural-chat-7b-v3-1'\n# model_path = \"google/gemma-1.1-7b-it\"\n\nquantization_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_compute_dtype=torch.float16,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_use_double_quant=True,\n)\n\ndef get_mistral():\n    tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n    model = AutoModelForCausalLM.from_pretrained(\n        model_path,\n        device_map=\"auto\",\n        quantization_config=quantization_config,\n        trust_remote_code=True,\n    )\n    return tokenizer, model\n\ndef get_llama():\n    tokenizer = AutoTokenizer.from_pretrained(model_path)\n    model_4bit = AutoModelForCausalLM.from_pretrained(\n        model_path,\n#         \"SweatyCrayfish/llama-3-8b-quantized\", \n#         load_in_4bit=True, \n#         torch_dtype=torch.float16,\n        device_map=\"auto\",\n        quantization_config=quantization_config,\n    )\n    return tokenizer, model_4bit\n\ntokenizer, model = get_llama()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Dataset\n\n- Skinuti s [linka](https://drive.google.com/drive/u/0/folders/1kiFXp5fqpf8--NQJJAlBIBpfSaXTk1UY)\n\n- Staviti u folder `/data`, ako na Kaggleu, onda napraviti svoj dataset te ga dodati pod input","metadata":{}},{"cell_type":"code","source":"# few_shot_examples = list(np.load(\"./{dataset_path}/data/demonstration.npy\",allow_pickle=True))\ndata_path = '/kaggle/input/brainteaser/data'\nsentence_data_path = f\"{data_path}/SP-train.npy\"\nwordplay_data_list = f\"{data_path}/WP-train.npy\"\nsentence_data_list = list(np.load(sentence_data_path,allow_pickle=True))\nwordplay_data_list = list(np.load(wordplay_data_list,allow_pickle=True))\n\ntest_data_list = sentence_data_list + wordplay_data_list\nprint(f\"Dataset length {len(test_data_list)}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Format za mystral, za ostale modele maknuti linije `<s>[INST]`","metadata":{}},{"cell_type":"code","source":"format = \"\"\ndef get_format():\n    global format, letters\n    newline = '\\n'\n    format = f\"\"\"Question: {\"{}\"}\n\nChoices:\n{''.join('(' + x + ') {}' + newline for x in letters)}\"\"\"\n    return format\n\ndef get_llama_format(demonstration=None):\n    global tokenizer, format\n    messages = []\n    if demonstration is not None:\n        messages.append({\"role\": \"system\", \"content\": demonstration})\n    messages.append({\"role\": \"user\", \"content\": get_format()})\n    messages.append({\"role\": \"assistant\", \"content\": \"Answer:(\"})\n    try:\n        format = tokenizer.apply_chat_template(messages, tokenize=False)\n    except Exception:\n        # When using mistral model, roles in messages must alternate, this way it works without errors\n        messages[1][\"content\"] = demonstration + \"\\n\\n\" + messages[1][\"content\"]\n        format = tokenizer.apply_chat_template(messages[1:], tokenize=False)\n    # remove \"<|eot_id|>\" from the end\n    format = format[:format.index(\"Answer:(\")+8]\n\ndef get_appended_format(demonstration=None):\n    global format\n    format = get_format()\n    if demonstration is not None:\n        format = demonstration + \"\\n\\n\" + format\n    format += \"\\nAnswer:(\"\n\n# get_llama_format(\"Primjer\")\nset_letters(4)\nget_appended_format()\nprint(format)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_single_demo(sample):\n    global format\n    sample_demo = format.format(\n        sample['question'], *sample['choice_list'])\n    return sample_demo","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Generiranje dataseta\n\n### Pomocne funkcije","metadata":{}},{"cell_type":"code","source":"def set_predictions(data_list):\n    \"\"\"Uzima response i parsira ga tako da pod predict stavi slovo koje je napisano\"\"\"\n    global letters\n    for index,item in enumerate(data_list):\n        item['predict'] = None\n        for x in letters:\n            if (f'{x})') in item['response']:\n                item['predict'] = x\n\n        if item['predict'] is None:\n            print(index)\n\ndef custom_stopping_criteria(input_ids: torch.LongTensor, score: torch.FloatTensor, **kwargs) -> bool:\n    \"\"\"\n    Funkcija kako bi se dinamicki prepoznalo kad treba prestati generirati tekst:\n    ne nakon fiksnog broja tokena, vec kad model napise rjesenje. Najcesce\n    ce to ipak biti medu prva dva tokena\n    \"\"\"\n    global good_responses\n    decoded = tokenizer.decode(input_ids[0][-3:])\n    for good_response in good_responses:\n        if good_response in decoded:\n            return True\n    return False\n\nstopping_criteria = StoppingCriteriaList([custom_stopping_criteria])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def generate(samples, tokens=10, all_tokens=True, few_shot=False):\n    global model\n    for sample in tqdm(samples):\n        # Prepare the input text\n        if few_shot:\n            text = demonstration + get_single_demo(sample)\n        else:\n            text = get_single_demo(sample)\n        \n        # Encode the input using the tokenizer\n        inputs = tokenizer.encode(text, return_tensors=\"pt\", return_attention_mask=False).to(device)\n        \n        # Generate the output text\n        with torch.no_grad():\n            results = model(inputs)\n            # Get the logits and calculate the softmax probabilities for the last token\n            logits = results.logits.squeeze()[-1]\n            probs = torch.nn.functional.softmax(logits, dim=-1)  # Apply softmax to get probabilities\n            sample['probs'] = probs  # Store probabilities in the sample\n\n        # Generate the final response (text)\n        outputs = model.generate(\n            inputs,\n            pad_token_id=tokenizer.eos_token_id,\n            do_sample=False,\n            max_new_tokens=tokens,\n            stopping_criteria=stopping_criteria\n        )\n        \n        # Extract the generated output\n        original_tokens = len(inputs[0])\n        generated_output = outputs[0][0 if all_tokens else original_tokens:]\n        response = tokenizer.decode(generated_output, skip_special_tokens=True)\n        \n        # Store the response in the sample\n        sample['response'] = response\n    \n    # Process the predictions (e.g., based on the model outputs)\n    set_predictions(samples)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(test_data_list[0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def turn_to_gibberish():\n    output_name = \"gibberish_all\"\n    \n    random.seed(10)\n    for dict in test_data_list:\n        choice_list = dict[\"choice_list\"]\n        for i in range(len(choice_list)):\n            if i != dict[\"label\"]:\n                s = choice_list[i]\n                choice_list[i] = ''.join(random.sample(s,len(s)))\n        \n        if dict[\"id\"] == \"SP-88\":\n            for key,item in dict.items():\n                print(f\"{key}: {item}\")\n    \n    return test_data_list, output_name\ngenerated_data_list, output_name = turn_to_gibberish()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"generate(test_data_list, tokens=20, all_tokens=False, few_shot=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_probs(data, dict_matrix, threshold):\n    letters = ['a', 'A', 'b', 'B', 'c', 'C', 'd', 'D']\n    total_probs = {}  # Dictionary to store the cumulative probabilities for each letter\n    count = 0\n    total_prob = 0\n    \n    probs = data['probs']  # Assuming this contains probabilities from the model\n    cond = probs > probs.max() * 0.0001  # Filter out very low probabilities\n    indices = cond.nonzero()  # Get indices of significant probabilities\n    values = probs[cond]  # Get the values of those probabilities\n    \n    # Sum the probabilities for each relevant letter\n    for ind, val in zip(indices[:, 0], values):\n        letter = tokenizer.decode(ind)\n        if letter not in total_probs:\n            total_probs[letter] = 0  # Initialize the letter if not already present\n        total_probs[letter] += float(val)  # Add the probability for the letter\n        total_prob += val  # Sum up the total probabilities\n    \n    # Normalize total_probs by dividing each by total_prob\n    if float(total_prob) > 0:  # Prevent division by zero\n        for letter in total_probs:\n            total_probs[letter] /= total_prob  # Normalize each probability\n            \n    sorted_total_probs = sorted(total_probs.items(), key=lambda x: x[1], reverse=True)\n    letters = ['A', 'B', 'C', 'D']\n    choice_to_index = {letters[i]: i for i in range(len(letters))}\n    choice_to_index[None] = len(letters)\n    if choice_to_index[data['predict']] == data['label']:\n        let, prob = sorted_total_probs[0]\n        if prob >= threshold:\n            dict_matrix['correct_high'].append(1)\n        else:\n            dict_matrix['correct_low'].append(1)\n    else:\n        let, prob = sorted_total_probs[0]\n        if prob > threshold:\n            dict_matrix['incorrect_high'].append(1)\n        else:\n            dict_matrix['incorrect_low'].append(1)\n    return","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dict_matrix = {}\ndict_matrix['correct_high'] = []\ndict_matrix['correct_low'] = []\ndict_matrix['incorrect_high'] = []\ndict_matrix['incorrect_low'] = []\n\nfor item in test_data_list:\n    get_probs(item, dict_matrix, 0.5)\nsuma = 0\nfor k, v in dict_matrix.items():\n    suma += sum(v)\nfor k, v in dict_matrix.items():\n    dict_matrix[k] = round(sum(v) / suma, 3)\nprint(dict_matrix)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"word_play, sentence_play = getResultdata(test_data_list)\nfinal_result = getSeperateResult(word_play, sentence_play)\nfinal_result.update(dict_matrix)  # This merges dict_matrix into final_result\nsave(final_result, name=\"original\", example=get_single_demo(test_data_list[0]), data=test_data_list)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}