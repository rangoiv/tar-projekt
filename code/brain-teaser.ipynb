{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Brain teaser\n","\n","[Original Github REPO](https://github.com/1171-jpg/BrainTeaser)\n","\n","[Leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard)\n"]},{"cell_type":"markdown","metadata":{},"source":["### Importing required functions"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-05T20:01:50.748616Z","iopub.status.busy":"2024-06-05T20:01:50.748378Z","iopub.status.idle":"2024-06-05T20:02:24.483959Z","shell.execute_reply":"2024-06-05T20:02:24.483028Z","shell.execute_reply.started":"2024-06-05T20:01:50.748592Z"},"trusted":true},"outputs":[],"source":["!pip install -q -U langchain transformers bitsandbytes accelerate"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-05T20:02:24.485991Z","iopub.status.busy":"2024-06-05T20:02:24.485689Z","iopub.status.idle":"2024-06-05T20:02:42.391397Z","shell.execute_reply":"2024-06-05T20:02:42.390481Z","shell.execute_reply.started":"2024-06-05T20:02:24.485963Z"},"trusted":true},"outputs":[],"source":["import numpy as np\n","import json\n","from datasets import load_dataset\n","from random import shuffle\n","import random\n","\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import seaborn as sns\n","\n","import torch\n","from torch.nn.functional import normalize\n","from transformers import AutoModel, AutoTokenizer\n","\n","from tqdm.auto import tqdm\n","import numpy as np\n","import os\n","from transformers import StoppingCriteriaList\n","\n","from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline\n","import torch\n","from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, AutoModelForCausalLM\n","import gc"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-05T20:02:42.392986Z","iopub.status.busy":"2024-06-05T20:02:42.392484Z","iopub.status.idle":"2024-06-05T20:02:42.399237Z","shell.execute_reply":"2024-06-05T20:02:42.398429Z","shell.execute_reply.started":"2024-06-05T20:02:42.392961Z"},"trusted":true},"outputs":[],"source":["good_responses, letters = None, None\n","def set_letters(k):\n","    global letters, good_responses\n","    letters = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P'][:k]\n","    good_responses = [f'{x})' for x in letters]\n","set_letters(4)\n","print(letters)\n","print(good_responses)"]},{"cell_type":"markdown","metadata":{},"source":["### Metrics\n","\n","Taken from the repository of the original authors of the dataset: https://github.com/1171-jpg/BrainTeaser/tree/main"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":false,"execution":{"iopub.execute_input":"2024-06-05T20:02:42.401421Z","iopub.status.busy":"2024-06-05T20:02:42.401164Z","iopub.status.idle":"2024-06-05T20:02:42.427937Z","shell.execute_reply":"2024-06-05T20:02:42.427061Z","shell.execute_reply.started":"2024-06-05T20:02:42.401398Z"},"trusted":true},"outputs":[],"source":["import json\n","from tqdm import tqdm\n","import torch\n","import logging\n","import argparse\n","import numpy as np\n","\n","def getResultdata(result_data):\n","    global letters\n","    choice_to_index = {letters[i]: i for i in range(len(letters))}\n","    choice_to_index[None] = len(letters)\n","\n","    word_play = {}\n","    reverse_play = {}\n","    for item in result_data:\n","        item_type = item['id'].split(\"-\")[0]\n","        item_id = item['id'].split(\"-\")[1].split(\"_\")[0]\n","        if item_type == 'WP':\n","            if item_id not in word_play:\n","                word_play[item_id] = [0,0,0]\n","        else:\n","            if item_id not in reverse_play:\n","                reverse_play[item_id] = [0,0,0]\n","\n","    for item in result_data:\n","        item_type = item['id'].split(\"-\")[0]\n","        item_id = item['id'].split(\"-\")[1].split(\"_\")[0]\n","        ad_type = 0\n","        if 'SR' in item['id']:\n","            ad_type = 1\n","        elif 'CR' in item['id']:\n","            ad_type = 2\n","        else:\n","            ad_type = 0\n","\n","        if item_type == 'WP':\n","            if choice_to_index[item['predict']] == item['label']:\n","                word_play[item_id][ad_type] = 1\n","        else:\n","            if choice_to_index[item['predict']] == item['label']:\n","                reverse_play[item_id][ad_type] = 1\n","                \n","    return word_play,reverse_play\n","\n","\n","def getMetric(data_list):\n","    data_list = np.array(data_list)\n","    overall_accuracy = np.sum(data_list)/3/len(data_list)\n","    original_accuracy = np.sum(data_list,axis = 0)[0]/len(data_list)\n","    semantic_accuracy = np.sum(data_list,axis = 0)[1]/len(data_list)\n","    context_accuracy = np.sum(data_list,axis = 0)[2]/len(data_list)\n","    ori_sema = np.sum([1 if item[0]==1 and item[1] == 1 else 0 for item in data_list])/len(data_list)\n","    ori_sema_cont = np.sum([1 if item[0]==1 and item[1] == 1 and item[2] == 1  else 0 for item in data_list])/len(data_list)\n","    \n","    print(\"over_all accuracy {}\".format(overall_accuracy))\n","    print(\"single_original_accuracy {}\".format(original_accuracy))\n","    print(\"single_semantic_accuracy {}\".format(semantic_accuracy))\n","    print(\"single_context_accuracy {}\".format(context_accuracy))\n","    print(\"sr_accuracy {}\".format(ori_sema))\n","    print(\"cr_accuracy {}\".format(ori_sema_cont))\n","\n","    return {'over_all accuracy':overall_accuracy,'original_accuracy':original_accuracy,'semantic_accuracy':semantic_accuracy,'context_accuracy':context_accuracy,'ori_sema':ori_sema,'ori_sema_cont':ori_sema_cont}\n","\n","\n","def getSeperateResult(word_play,reverse_thinking):\n","    final_result = {}\n","    word_data_list = []\n","    word_data_list = list(word_play.values())\n","    print('#########Wordplay##########')\n","    final_result['wordplay'] = getMetric(word_data_list)\n","    \n","    reverse_data_list = []\n","    for item in reverse_thinking.values():\n","        reverse_data_list.append(item)\n","    print('#########Sentence##########')   \n","    final_result['sentence'] = getMetric(reverse_data_list)  \n","    \n","    \n","    all_data = word_data_list + reverse_data_list\n","    print('#########All data##########') \n","    final_result['all'] = getMetric(all_data) \n","    \n","    return final_result"]},{"cell_type":"markdown","metadata":{},"source":["### Data saving\n","\n","The file name format follows: model_name + name + serial number.\n","\n","Automatically appends +1 to the name if a file with the same name already exists.\n","\n","Inside, it writes the first test example and the results."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-05T20:02:42.429498Z","iopub.status.busy":"2024-06-05T20:02:42.429087Z","iopub.status.idle":"2024-06-05T20:02:42.444119Z","shell.execute_reply":"2024-06-05T20:02:42.443299Z","shell.execute_reply.started":"2024-06-05T20:02:42.429467Z"},"trusted":true},"outputs":[],"source":["dirname= 'results'\n","pickle_dir = 'pickled_results'\n","\n","def create_dir(name):\n","    try:\n","        os.mkdir(name)\n","    except:\n","        pass\n","\n","def save(final_result, name=None, few_shot=False, example=format, data=None):\n","    create_dir(dirname)\n","    create_dir(pickle_dir)\n","    \n","    model_name = model_path.split('/')[-1]\n","    inserted_name = \"\" if name is None else \"_\" + name\n","    i = 1\n","    while True:\n","        file_path = f'{dirname}/{model_name}{inserted_name}_{i}.txt'\n","        pickle_path = f'{pickle_dir}/{model_name}{inserted_name}_{i}.json'\n","        try:\n","            with open(file_path, 'r'):\n","                i += 1\n","        except:\n","            with open(file_path, 'w') as file:\n","                file.write(model_name+'\\n')\n","                if few_shot:\n","                    file.write(few_shot + '\\n')\n","                if not isinstance(example, str):\n","                    example = json.dumps(example)\n","                file.write(example + '\\n')\n","                file.write(json.dumps(final_result, indent=4))\n","            if data is None:\n","                print(f\"Warning, {model_name} results data not saving in json, please provide data arg\")\n","            else:\n","                with open(pickle_path, 'w') as file:\n","                    file.write(json.dumps(data, indent=4))\n","            return"]},{"cell_type":"markdown","metadata":{},"source":["### Model loading\n","\n","Login to HuggingFace account to load advanced models."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-05T20:02:42.445496Z","iopub.status.busy":"2024-06-05T20:02:42.445178Z","iopub.status.idle":"2024-06-05T20:02:42.485681Z","shell.execute_reply":"2024-06-05T20:02:42.484847Z","shell.execute_reply.started":"2024-06-05T20:02:42.445465Z"},"trusted":true},"outputs":[],"source":["device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","device"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-05T20:02:42.487421Z","iopub.status.busy":"2024-06-05T20:02:42.487171Z","iopub.status.idle":"2024-06-05T20:02:42.623781Z","shell.execute_reply":"2024-06-05T20:02:42.622967Z","shell.execute_reply.started":"2024-06-05T20:02:42.487400Z"},"trusted":true},"outputs":[],"source":["from huggingface_hub import login\n","login('', add_to_git_credential=False, write_permission=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-05T20:02:42.630066Z","iopub.status.busy":"2024-06-05T20:02:42.629821Z","iopub.status.idle":"2024-06-05T20:05:17.643301Z","shell.execute_reply":"2024-06-05T20:05:17.642569Z","shell.execute_reply.started":"2024-06-05T20:02:42.630045Z"},"trusted":true},"outputs":[],"source":["# model_path = 'mistralai/Mistral-7B-Instruct-v0.2'\n","# model_path = 'microsoft/Phi-3-mini-128k-instruct'\n","# model_path = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n","model_path = \"google/gemma-1.1-7b-it\"\n","\n","quantization_config = BitsAndBytesConfig(\n","    load_in_4bit=True,\n","    bnb_4bit_compute_dtype=torch.float16,\n","    bnb_4bit_quant_type=\"nf4\",\n","    bnb_4bit_use_double_quant=True,\n",")\n","\n","def get_model():\n","    tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n","    model = AutoModelForCausalLM.from_pretrained(\n","        model_path,\n","        device_map=\"auto\",\n","        quantization_config=quantization_config,\n","        trust_remote_code=True,\n","    )\n","    return tokenizer, model\n","\n","tokenizer, model = get_model()"]},{"cell_type":"markdown","metadata":{},"source":["### Dataset\n","\n","- Taken from original authors link: https://drive.google.com/drive/u/0/folders/1kiFXp5fqpf8--NQJJAlBIBpfSaXTk1UY"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-05T20:05:17.647435Z","iopub.status.busy":"2024-06-05T20:05:17.647158Z","iopub.status.idle":"2024-06-05T20:05:17.687170Z","shell.execute_reply":"2024-06-05T20:05:17.686368Z","shell.execute_reply.started":"2024-06-05T20:05:17.647410Z"},"trusted":true},"outputs":[],"source":["data_path = '/kaggle/input/brainteaser/data'\n","sentence_data_path = f\"{data_path}/SP-train.npy\"\n","wordplay_data_list = f\"{data_path}/WP-train.npy\"\n","sentence_data_list = list(np.load(sentence_data_path,allow_pickle=True))\n","wordplay_data_list = list(np.load(wordplay_data_list,allow_pickle=True))\n","\n","test_data_list = sentence_data_list + wordplay_data_list\n","print(f\"Dataset length {len(test_data_list)}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-05T20:05:17.689008Z","iopub.status.busy":"2024-06-05T20:05:17.688446Z","iopub.status.idle":"2024-06-05T20:05:17.698763Z","shell.execute_reply":"2024-06-05T20:05:17.697941Z","shell.execute_reply.started":"2024-06-05T20:05:17.688976Z"},"trusted":true},"outputs":[],"source":["format = \"\"\n","def get_format():\n","    global format, letters\n","    newline = '\\n'\n","    format = f\"\"\"Question: {\"{}\"}\n","\n","Choices:\n","{''.join('(' + x + ') {}' + newline for x in letters)}\"\"\"\n","    return format\n","\n","def get_llama_format(demonstration=None):\n","    global tokenizer, format\n","    messages = []\n","    if demonstration is not None:\n","        messages.append({\"role\": \"system\", \"content\": demonstration})\n","    messages.append({\"role\": \"user\", \"content\": get_format()})\n","    messages.append({\"role\": \"assistant\", \"content\": \"Answer:(\"})\n","    try:\n","        format = tokenizer.apply_chat_template(messages, tokenize=False)\n","    except Exception:\n","        # When using mistral model, roles in messages must alternate, this way it works without errors\n","        messages[1][\"content\"] = demonstration + \"\\n\\n\" + messages[1][\"content\"]\n","        format = tokenizer.apply_chat_template(messages[1:], tokenize=False)\n","    # remove \"<|eot_id|>\" from the end\n","    format = format[:format.index(\"Answer:(\")+8]\n","\n","def get_appended_format(demonstration=None):\n","    global format\n","    format = get_format()\n","    if demonstration is not None:\n","        format = demonstration + \"\\n\\n\" + format\n","    format += \"\\nAnswer:(\"\n","\n","# get_llama_format(\"Primjer\")\n","set_letters(4)\n","get_appended_format()\n","print(format)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["format = \"\"\"The following question is a brainteaser.\n","\n","Question: {}\n","\n","Choices:\n","(A) {}\n","(B) {}\n","(C) {}\n","(D) {}\n","\n","Answer:(\"\"\""]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-05T20:05:17.700415Z","iopub.status.busy":"2024-06-05T20:05:17.699995Z","iopub.status.idle":"2024-06-05T20:05:17.707635Z","shell.execute_reply":"2024-06-05T20:05:17.706911Z","shell.execute_reply.started":"2024-06-05T20:05:17.700384Z"},"trusted":true},"outputs":[],"source":["def get_single_demo(sample):\n","    global format\n","    sample_demo = format.format(\n","        sample['question'], *sample['choice_list'])\n","    return sample_demo"]},{"cell_type":"markdown","metadata":{},"source":["### Utility functions"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-05T20:05:17.708830Z","iopub.status.busy":"2024-06-05T20:05:17.708556Z","iopub.status.idle":"2024-06-05T20:05:17.718970Z","shell.execute_reply":"2024-06-05T20:05:17.718154Z","shell.execute_reply.started":"2024-06-05T20:05:17.708807Z"},"trusted":true},"outputs":[],"source":["def set_predictions(data_list):\n","    \"\"\"Uzima response i parsira ga tako da pod predict stavi slovo koje je napisano\"\"\"\n","    global letters\n","    for index,item in enumerate(data_list):\n","        item['predict'] = None\n","        for x in letters:\n","            if (f'{x})') in item['response']:\n","                item['predict'] = x\n","\n","        if item['predict'] is None:\n","            print(index)\n","\n","def custom_stopping_criteria(input_ids: torch.LongTensor, score: torch.FloatTensor, **kwargs) -> bool:\n","    \"\"\"\n","    Funkcija kako bi se dinamicki prepoznalo kad treba prestati generirati tekst:\n","    ne nakon fiksnog broja tokena, vec kad model napise rjesenje. Najcesce\n","    ce to ipak biti medu prva dva tokena\n","    \"\"\"\n","    global good_responses\n","    decoded = tokenizer.decode(input_ids[0][-3:])\n","    for good_response in good_responses:\n","        if good_response in decoded:\n","            return True\n","    return False\n","\n","stopping_criteria = StoppingCriteriaList([custom_stopping_criteria])"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-05T20:05:17.720362Z","iopub.status.busy":"2024-06-05T20:05:17.720018Z","iopub.status.idle":"2024-06-05T20:05:17.734525Z","shell.execute_reply":"2024-06-05T20:05:17.733713Z","shell.execute_reply.started":"2024-06-05T20:05:17.720330Z"},"trusted":true},"outputs":[],"source":["def generate(samples, tokens=10, all_tokens=True, few_shot=False):\n","    global model\n","    for sample in tqdm(samples):\n","        if few_shot:\n","            text = demonstration + get_single_demo(sample)\n","        else:\n","            text = get_single_demo(sample)\n","        inputs = tokenizer.encode(text, return_tensors=\"pt\", return_attention_mask=False).to(device)\n","        original_tokens = len(inputs[0])\n","        outputs = model.generate(\n","            inputs,\n","            pad_token_id=tokenizer.eos_token_id,\n","            do_sample = False,\n","            max_new_tokens=tokens,\n","            stopping_criteria=stopping_criteria\n","        )\n","        outputs = outputs[0][0 if all_tokens else original_tokens:]\n","        sample['response'] = tokenizer.decode(outputs)\n","    set_predictions(samples)"]},{"cell_type":"markdown","metadata":{},"source":["## Experiments"]},{"cell_type":"markdown","metadata":{},"source":["### Prompt modifications"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["demonstration1 = \"Please pick the best choice for the brain teaser. Each brain teaser has only one possible solution including the choice none of above, answer should only provide the choice:\"\n","demonstration2 = \"The following question is a brainteaser. One choice is correct, other choices are semanticaly derived from the question.\"\n","demonstration3 = \"The following question is a brainteaser.\"\n","demonstration4 = \"The following question is a brainteaser. One choice is correct, other choices are semanticaly derived from the question. Sometimes none of the above is correct.\"\n","demonstration5 = \"The following question is a brainteaser. Sometimes none of the above is correct.\""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def generate_with_demonstration(demonstration=None,test_data_list=test_data_list, name=\"with-instruction\", llama_format=False):\n","    if llama_format:\n","        get_llama_format(demonstration)\n","    else:\n","        get_appended_format(demonstration)\n","    generate(test_data_list[:10], tokens=1000, all_tokens=False, few_shot=False)\n","    word_play,sentence_play = getResultdata(test_data_list)\n","    final_result = getSeperateResult(word_play, sentence_play)\n","    save(final_result, name=name, example=get_single_demo(test_data_list[0]), data=test_data_list)"]},{"cell_type":"markdown","metadata":{},"source":["### Mutiple answers"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-05T20:05:25.885444Z","iopub.status.busy":"2024-06-05T20:05:25.885079Z","iopub.status.idle":"2024-06-05T20:05:25.897960Z","shell.execute_reply":"2024-06-05T20:05:25.896899Z","shell.execute_reply.started":"2024-06-05T20:05:25.885420Z"},"trusted":true},"outputs":[],"source":["def create_with_k_questions(i, k):\n","    global test_data_list\n","    current = test_data_list[i]\n","    choice_list = [*current['choice_list']]\n","    choice_order = [*current['choice_order']]\n","    while len(choice_list) < k-1:\n","        i -= 11\n","        previous = test_data_list[i]\n","        choice_list.insert(-1, previous['distractor1'])\n","        choice_list.insert(-1, previous['distractor2'])\n","    last_order = choice_list[-1]\n","    choice_list = choice_list[:-1]\n","#     print(current['answer'])\n","    shuffle(choice_list)\n","    choice_list.append(last_order)\n","    try:\n","        label = choice_list.index(current['answer'])\n","    except Exception as e:\n","        print(current['answer'])\n","        label = len(choice_list)-1\n","    new = {**current}\n","    new['label'] = label\n","    new['choice_list'] = choice_list\n","    return new\n","\n","def test_with_k_questions(generated_data_list):\n","    for original, new in zip(test_data_list, generated_data_list):\n","        assert set(original['choice_list']).issubset(set(new['choice_list'])), str(original) + '\\n'+ str(new)\n","\n","def create_list_with_k_questions(k):\n","    global letters\n","    random.seed(10)\n","    generated_data_list = []\n","    for i in range(len(test_data_list)):\n","        new = create_with_k_questions(i, len(letters))\n","        generated_data_list.append(new)\n","    test_with_k_questions(generated_data_list)\n","    return generated_data_list"]},{"cell_type":"markdown","metadata":{},"source":["### Fixed position of correct answer"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def create_with_ordered_correct_answer(i, target_position):\n","    current = test_data_list[i]\n","    new = {**current}\n","    choice_list = [*current['choice_list']]\n","    correct_answer = choice_list[current[\"label\"]]\n","    choice_list.remove(correct_answer)\n","    choice_list.insert(target_position,correct_answer)\n","    new['choice_list'] = choice_list\n","    new['label'] = target_position\n","    return new\n","\n","generated_data_list = []\n","for i in range(len(test_data_list)):\n","    new = create_with_ordered_correct_answer(i, 3)\n","    generated_data_list.append(new)\n","    "]},{"cell_type":"markdown","metadata":{},"source":["### Repeated distractors"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def all_same_incorrect_answers(i):\n","    current = test_data_list[i]\n","    new = {**current}\n","    choice_list = [*current['choice_list']]\n","    correct_answer = choice_list[current[\"label\"]]\n","    choice_list.remove(correct_answer)\n","    incorrect_answer = choice_list[0]\n","    choice_list.clear()\n","    choice_list.insert(0,correct_answer)\n","    choice_list.insert(1,incorrect_answer)\n","    choice_list.insert(2,incorrect_answer)\n","    choice_list.insert(3,incorrect_answer)\n","    random.shuffle(choice_list)\n","    \n","    new['choice_list'] = choice_list\n","    new['label'] = choice_list.index(correct_answer)\n","    return new\n","\n","generated_data_list = []\n","for i in range(len(test_data_list)):\n","    new = all_same_incorrect_answers(i)\n","    generated_data_list.append(new)\n","\n","# Save the list to a file\n","import pickle\n","with open('generated_data_list.pkl', 'wb') as file:\n","    pickle.dump(generated_data_list, file)\n","\n","# Loading the generated list to ensure consistency across all models (since we use random.shuffle)\n","with open('/kaggle/input/generated-data-list/generated_data_list.pkl', 'rb') as file:\n","    generated_data_list = pickle.load(file)"]},{"cell_type":"markdown","metadata":{},"source":["### Replace one distractor with question"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def replace_distractor_with_question():\n","    output_name = \"replace_distractor_with_question\"\n","    \n","    test_data_list = load_dataset()\n","\n","    for dict in test_data_list:\n","        dict[\"distractor1\"] = dict[\"question\"]\n","        distractor1_index = dict[\"choice_order\"].index(1)\n","        choice_list = dict[\"choice_list\"]\n","        choice_list[distractor1_index] = dict[\"question\"]\n","    \n","    return test_data_list, output_name\n","generated_data_list, output_name = replace_distractor_with_question()"]},{"cell_type":"markdown","metadata":{},"source":["### Shuffled text (gibberish)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def turn_to_gibberish():\n","    output_name = \"gibberish_all\"\n","    \n","    test_data_list = load_dataset()\n","\n","    random.seed(10)\n","    for dict in test_data_list:\n","        choice_list = dict[\"choice_list\"]\n","        for i in range(len(choice_list)):\n","            if i != dict[\"label\"]:\n","                s = choice_list[i]\n","                choice_list[i] = ''.join(random.sample(s,len(s)))\n","        \n","        if dict[\"id\"] == \"SP-88\":\n","            for key,item in dict.items():\n","                print(f\"{key}: {item}\")\n","    \n","    return test_data_list, output_name\n","generated_data_list, output_name = turn_to_gibberish()"]},{"cell_type":"markdown","metadata":{},"source":["## Generate and save"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["generate(generated_data_list, tokens=20, all_tokens=False, few_shot=False)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["word_play,sentence_play = getResultdata(generated_data_list)\n","final_result = getSeperateResult(word_play, sentence_play)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["save(final_result, name=\"\", example=get_single_demo(generated_data_list[0]), data=generated_data_list)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Cleanup before loading new model\n","import gc\n","del model\n","model = None\n","gc.collect()\n","torch.cuda.empty_cache()"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":4946147,"sourceId":8329940,"sourceType":"datasetVersion"}],"dockerImageVersionId":30699,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
