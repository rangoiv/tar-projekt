{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8329940,"sourceType":"datasetVersion","datasetId":4946147},{"sourceId":8393115,"sourceType":"datasetVersion","datasetId":4992831}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Brain teaser","metadata":{}},{"cell_type":"code","source":"!pip install -q -U langchain transformers bitsandbytes accelerate","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport json\nfrom datasets import load_dataset\nfrom random import shuffle\nimport random\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\n\nimport torch\nfrom torch.nn.functional import normalize\nfrom transformers import AutoModel, AutoTokenizer\n\nfrom tqdm.auto import tqdm\nimport numpy as np\nimport os\nfrom transformers import StoppingCriteriaList\n\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline\nimport torch","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Ako imamo vise ponudenih odgovora od 4, da postupak bude automatski u svim funkcijama\nletters = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P'][:4]\nletters, len(letters)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import json\nfrom tqdm import tqdm\nimport torch\nimport logging\nimport argparse\nimport numpy as np\n\ndef getResultdata(result_data):\n    choice_to_index = {letters[i]: i for i in range(len(letters))}\n    choice_to_index[None] = len(letters)\n\n    word_play = {}\n    reverse_play = {}\n    for item in result_data:\n        item_type = item['id'].split(\"-\")[0]\n        item_id = item['id'].split(\"-\")[1].split(\"_\")[0]\n        if item_type == 'WP':\n            if item_id not in word_play:\n                word_play[item_id] = [0,0,0]\n        else:\n            if item_id not in reverse_play:\n                reverse_play[item_id] = [0,0,0]\n\n    for item in result_data:\n        item_type = item['id'].split(\"-\")[0]\n        item_id = item['id'].split(\"-\")[1].split(\"_\")[0]\n        ad_type = 0\n        if 'SR' in item['id']:\n            ad_type = 1\n        elif 'CR' in item['id']:\n            ad_type = 2\n        else:\n            ad_type = 0\n\n        if item_type == 'WP':\n            if choice_to_index[item['predict']] == item['label']:\n                word_play[item_id][ad_type] = 1\n        else:\n            if choice_to_index[item['predict']] == item['label']:\n                reverse_play[item_id][ad_type] = 1\n                \n    return word_play,reverse_play\n\n\ndef getMetric(data_list):\n    data_list = np.array(data_list)\n    overall_accuracy = np.sum(data_list)/3/len(data_list)\n    original_accuracy = np.sum(data_list,axis = 0)[0]/len(data_list)\n    semantic_accuracy = np.sum(data_list,axis = 0)[1]/len(data_list)\n    context_accuracy = np.sum(data_list,axis = 0)[2]/len(data_list)\n    ori_sema = np.sum([1 if item[0]==1 and item[1] == 1 else 0 for item in data_list])/len(data_list)\n    ori_sema_cont = np.sum([1 if item[0]==1 and item[1] == 1 and item[2] == 1  else 0 for item in data_list])/len(data_list)\n    \n    print(\"over_all accuracy {}\".format(overall_accuracy))\n    print(\"single_original_accuracy {}\".format(original_accuracy))\n    print(\"single_semantic_accuracy {}\".format(semantic_accuracy))\n    print(\"single_context_accuracy {}\".format(context_accuracy))\n    print(\"sr_accuracy {}\".format(ori_sema))\n    print(\"cr_accuracy {}\".format(ori_sema_cont))\n\n    return {'over_all accuracy':overall_accuracy,'original_accuracy':original_accuracy,'semantic_accuracy':semantic_accuracy,'context_accuracy':context_accuracy,'ori_sema':ori_sema,'ori_sema_cont':ori_sema_cont}\n\n\ndef getSeperateResult(word_play,reverse_thinking):\n    final_result = {}\n    word_data_list = []\n    word_data_list = list(word_play.values())\n    print('#########Wordplay##########')\n    final_result['wordplay'] = getMetric(word_data_list)\n    \n    reverse_data_list = []\n    for item in reverse_thinking.values():\n        reverse_data_list.append(item)\n    print('#########Sentence##########')   \n    final_result['sentence'] = getMetric(reverse_data_list)  \n    \n    \n    all_data = word_data_list + reverse_data_list\n    print('#########All data##########') \n    final_result['all'] = getMetric(all_data) \n    \n    return final_result","metadata":{"_kg_hide-input":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def save(final_result, name=None, few_shot=False, example=format):\n    try:\n        os.mkdir('results')\n    except:\n        pass\n    model_name = model_path.split('/')[-1]\n    inserted_name = \"\" if name is None else \"_\" + name\n    i = 1\n    while True:\n        file_path = f'results/{model_name}{inserted_name}_{i}.txt'\n        try:\n            with open(file_path, 'r'):\n                i += 1\n        except:\n            with open(file_path, 'w') as file:\n                file.write(model_name+'\\n')\n                if few_shot:\n                    file.write(few_shot + '\\n')\n                if not isinstance(example, str):\n                    example = json.dumps(example)\n                file.write(example + '\\n')\n                file.write(json.dumps(final_result, indent=4))\n                return","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Dataset\n\n- Skinuti s [linka](https://drive.google.com/drive/u/0/folders/1kiFXp5fqpf8--NQJJAlBIBpfSaXTk1UY)\n\n- Staviti u folder `/data`","metadata":{}},{"cell_type":"code","source":"device = 'cuda' if torch.cuda.is_available() else 'cpu'\ndevice","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Load the model","metadata":{}},{"cell_type":"code","source":"from huggingface_hub import login\nlogin('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', add_to_git_credential=False, write_permission=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model_path = 'mistralai/Mistral-7B-Instruct-v0.2'\n# model_path = 'microsoft/Phi-3-mini-128k-instruct'\n# model_path = \"meta-llama/Meta-Llama-3-8B-Instruct\"\nmodel_path = \"tiiuae/falcon-7b-instruct\"\n\nquantization_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_compute_dtype=torch.float16,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_use_double_quant=True,\n)\n\ndef get_mistral():\n    tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n    model = AutoModelForCausalLM.from_pretrained(\n        model_path,\n        device_map=\"auto\",\n        quantization_config=quantization_config,\n        trust_remote_code=True,\n    )\n    return tokenizer, model\n\ndef get_llama():\n    tokenizer = AutoTokenizer.from_pretrained(model_path)\n    model_4bit = AutoModelForCausalLM.from_pretrained(\n        model_path,\n#         \"SweatyCrayfish/llama-3-8b-quantized\", \n#         load_in_4bit=True, \n#         torch_dtype=torch.float16,\n        device_map=\"auto\",\n        quantization_config=quantization_config,\n    )\n    return tokenizer, model_4bit\n\ntokenizer, model = get_mistral()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from transformers import GPT2LMHeadModel, GPT2Tokenizer\n# import torch\n\n# model_path=\"gpt-2\"\n\n# # Load the model and tokenizer\n# model = GPT2LMHeadModel.from_pretrained(model_path)\n# tokenizer = GPT2Tokenizer.from_pretrained(model_path)\n\n# # Ensure the model is on the correct device\n# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# model.to(device)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_path = '/kaggle/input/brainteaser/data'\nsentence_data_path = f\"{data_path}/SP-train.npy\"\nwordplay_data_list = f\"{data_path}/WP-train.npy\"\nsentence_data_list = list(np.load(sentence_data_path,allow_pickle=True))\nwordplay_data_list = list(np.load(wordplay_data_list,allow_pickle=True))\n\ntest_data_list = sentence_data_list + wordplay_data_list\nprint(f\"Dataset length {len(test_data_list)}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data_list = sentence_data_list + wordplay_data_list\nprint(f\"Dataset length {len(test_data_list)}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"format = \"\"\"\nThe following question is a brainteaser.\n\nQuestion: {}\n\nChoices:\n(A) {}\n(B) {}\n(C) {}\n(D) {}\n\nAnswer:(\"\"\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_single_demo(sample):\n    sample_demo = format.format(\n        sample['question'], *sample['choice_list'])\n    return sample_demo","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(get_single_demo(test_data_list[0]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"good_responses = [f'{x})' for x in letters]\ngood_responses","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def set_predictions(data_list):\n    \"\"\"Uzima response i parsira ga tako da pod predict stavi slovo koje je napisano\"\"\"\n    for index,item in enumerate(data_list):\n        item['predict'] = None\n        for x in letters:\n            if (f'{x})') in item['response']:\n                item['predict'] = x\n\n        if item['predict'] is None:\n            print(index)\n\ndef custom_stopping_criteria(input_ids: torch.LongTensor, score: torch.FloatTensor, **kwargs) -> bool:\n    \"\"\"\n    Funkcija kako bi se dinamicki prepoznalo kad treba prestati generirati tekst:\n    ne nakon fiksnog broja tokena, vec kad model napise rjesenje. Najcesce\n    ce to ipak biti medu prva dva tokena\n    \"\"\"\n    decoded = tokenizer.decode(input_ids[0][-3:])\n    for good_response in good_responses:\n        if good_response in decoded:\n            return True\n    return False\n\nstopping_criteria = StoppingCriteriaList([custom_stopping_criteria])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def generate(samples, tokens=10, all_tokens=True, few_shot=False):\n    global model\n    for sample in tqdm(samples):\n        if few_shot:\n            text = demonstration + get_single_demo(sample)\n        else:\n            text = get_single_demo(sample)\n        inputs = tokenizer.encode(text, return_tensors=\"pt\", return_attention_mask=False).to(device)\n        original_tokens = len(inputs[0])\n        outputs = model.generate(\n            inputs,\n            pad_token_id=tokenizer.eos_token_id,\n            do_sample = False,\n            max_new_tokens=tokens,\n            stopping_criteria=stopping_criteria\n        )\n        outputs = outputs[0][0 if all_tokens else original_tokens:]\n        sample['response'] = tokenizer.decode(outputs)\n    set_predictions(samples)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_with_ordered_correct_answer(i, target_position):\n    current = test_data_list[i]\n    new = {**current}\n    choice_list = [*current['choice_list']]\n    correct_answer = choice_list[current[\"label\"]]\n    choice_list.remove(correct_answer)\n    choice_list.insert(target_position,correct_answer)\n    new['choice_list'] = choice_list\n    new['label'] = target_position\n    return new\n\ngenerated_data_list = []\nfor i in range(len(test_data_list)):\n    new = create_with_ordered_correct_answer(i, 1)\n    generated_data_list.append(new)\n    \nprint(generated_data_list[0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"generate(generated_data_list[:10], tokens=1000, all_tokens=False, few_shot=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for sample in test_data_list[:5]:\n    print(\"\\n>\", sample['response'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"generate(generated_data_list, tokens=20, all_tokens=False, few_shot=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"word_play,sentence_play = getResultdata(generated_data_list)\nfinal_result = getSeperateResult(word_play, sentence_play)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"save(final_result, name=\"always_B\", example=get_single_demo(test_data_list[0]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}