{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8329940,"sourceType":"datasetVersion","datasetId":4946147}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Brain teaser\n\n[Original Github REPO](https://github.com/1171-jpg/BrainTeaser)\n\n[Leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard)\n","metadata":{}},{"cell_type":"code","source":"!pip install -q -U langchain transformers bitsandbytes accelerate","metadata":{"execution":{"iopub.status.busy":"2024-05-27T19:46:55.022955Z","iopub.execute_input":"2024-05-27T19:46:55.023935Z","iopub.status.idle":"2024-05-27T19:47:28.606417Z","shell.execute_reply.started":"2024-05-27T19:46:55.023890Z","shell.execute_reply":"2024-05-27T19:47:28.605436Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncudf 23.8.0 requires cubinlinker, which is not installed.\ncudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ncudf 23.8.0 requires ptxcompiler, which is not installed.\ncuml 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ndask-cudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\nkeras-cv 0.8.2 requires keras-core, which is not installed.\nkeras-nlp 0.9.3 requires keras-core, which is not installed.\ntensorflow-decision-forests 1.8.1 requires wurlitzer, which is not installed.\napache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.8 which is incompatible.\napache-beam 2.46.0 requires numpy<1.25.0,>=1.14.3, but you have numpy 1.26.4 which is incompatible.\napache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 15.0.2 which is incompatible.\ncudf 23.8.0 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.4.0 which is incompatible.\ncudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.1.4 which is incompatible.\ncudf 23.8.0 requires protobuf<5,>=4.21, but you have protobuf 3.20.3 which is incompatible.\ncudf 23.8.0 requires pyarrow==11.*, but you have pyarrow 15.0.2 which is incompatible.\ncuml 23.8.0 requires dask==2023.7.1, but you have dask 2024.4.1 which is incompatible.\ndask-cuda 23.8.0 requires dask==2023.7.1, but you have dask 2024.4.1 which is incompatible.\ndask-cuda 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.1.4 which is incompatible.\ndask-cudf 23.8.0 requires dask==2023.7.1, but you have dask 2024.4.1 which is incompatible.\ndask-cudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.1.4 which is incompatible.\ndistributed 2023.7.1 requires dask==2023.7.1, but you have dask 2024.4.1 which is incompatible.\ngoogle-cloud-bigquery 2.34.4 requires packaging<22.0dev,>=14.3, but you have packaging 23.2 which is incompatible.\njupyterlab 4.1.6 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\njupyterlab-lsp 5.1.0 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\nlibpysal 4.9.2 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\nmomepy 0.7.0 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\nosmnx 1.9.2 requires shapely>=2.0, but you have shapely 1.8.5.post1 which is incompatible.\nraft-dask 23.8.0 requires dask==2023.7.1, but you have dask 2024.4.1 which is incompatible.\nspopt 0.6.0 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\ntensorflow 2.15.0 requires keras<2.16,>=2.15.0, but you have keras 3.2.1 which is incompatible.\nydata-profiling 4.6.4 requires numpy<1.26,>=1.16.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np\nimport json\nfrom datasets import load_dataset\nfrom random import shuffle\nimport random\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\n\nimport torch\nfrom torch.nn.functional import normalize\nfrom transformers import AutoModel, AutoTokenizer\n\nfrom tqdm.auto import tqdm\nimport numpy as np\nimport os\nfrom transformers import StoppingCriteriaList\n\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM, AutoModelForCausalLM\nimport gc","metadata":{"execution":{"iopub.status.busy":"2024-05-27T19:47:28.608277Z","iopub.execute_input":"2024-05-27T19:47:28.608575Z","iopub.status.idle":"2024-05-27T19:47:46.050067Z","shell.execute_reply.started":"2024-05-27T19:47:28.608544Z","shell.execute_reply":"2024-05-27T19:47:46.049306Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stderr","text":"2024-05-27 19:47:36.045744: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-05-27 19:47:36.045842: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-05-27 19:47:36.168097: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"# Ako imamo vise ponudenih odgovora od 4, da postupak bude automatski u svim funkcijama\nletters = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P'][:4]\nletters, len(letters)","metadata":{"execution":{"iopub.status.busy":"2024-05-27T19:47:46.051090Z","iopub.execute_input":"2024-05-27T19:47:46.051631Z","iopub.status.idle":"2024-05-27T19:47:46.063454Z","shell.execute_reply.started":"2024-05-27T19:47:46.051604Z","shell.execute_reply":"2024-05-27T19:47:46.061981Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"(['A', 'B', 'C', 'D'], 4)"},"metadata":{}}]},{"cell_type":"markdown","source":"## Metrike\n\nPreuzeto direktno s njihovog repoa","metadata":{}},{"cell_type":"code","source":"import json\nfrom tqdm import tqdm\nimport torch\nimport logging\nimport argparse\nimport numpy as np\n\ndef getResultdata(result_data):\n    choice_to_index = {letters[i]: i for i in range(len(letters))}\n    choice_to_index[None] = len(letters)\n\n    word_play = {}\n    reverse_play = {}\n    for item in result_data:\n        item_type = item['id'].split(\"-\")[0]\n        item_id = item['id'].split(\"-\")[1].split(\"_\")[0]\n        if item_type == 'WP':\n            if item_id not in word_play:\n                word_play[item_id] = [0,0,0]\n        else:\n            if item_id not in reverse_play:\n                reverse_play[item_id] = [0,0,0]\n\n    for item in result_data:\n        item_type = item['id'].split(\"-\")[0]\n        item_id = item['id'].split(\"-\")[1].split(\"_\")[0]\n        ad_type = 0\n        if 'SR' in item['id']:\n            ad_type = 1\n        elif 'CR' in item['id']:\n            ad_type = 2\n        else:\n            ad_type = 0\n\n        if item_type == 'WP':\n            if choice_to_index[item['predict']] == item['label']:\n                word_play[item_id][ad_type] = 1\n        else:\n            if choice_to_index[item['predict']] == item['label']:\n                reverse_play[item_id][ad_type] = 1\n                \n    return word_play,reverse_play\n\n\ndef getMetric(data_list):\n    data_list = np.array(data_list)\n    overall_accuracy = np.sum(data_list)/3/len(data_list)\n    original_accuracy = np.sum(data_list,axis = 0)[0]/len(data_list)\n    semantic_accuracy = np.sum(data_list,axis = 0)[1]/len(data_list)\n    context_accuracy = np.sum(data_list,axis = 0)[2]/len(data_list)\n    ori_sema = np.sum([1 if item[0]==1 and item[1] == 1 else 0 for item in data_list])/len(data_list)\n    ori_sema_cont = np.sum([1 if item[0]==1 and item[1] == 1 and item[2] == 1  else 0 for item in data_list])/len(data_list)\n    \n    print(\"over_all accuracy {}\".format(overall_accuracy))\n    print(\"single_original_accuracy {}\".format(original_accuracy))\n    print(\"single_semantic_accuracy {}\".format(semantic_accuracy))\n    print(\"single_context_accuracy {}\".format(context_accuracy))\n    print(\"sr_accuracy {}\".format(ori_sema))\n    print(\"cr_accuracy {}\".format(ori_sema_cont))\n\n    return {'over_all accuracy':overall_accuracy,'original_accuracy':original_accuracy,'semantic_accuracy':semantic_accuracy,'context_accuracy':context_accuracy,'ori_sema':ori_sema,'ori_sema_cont':ori_sema_cont}\n\n\ndef getSeperateResult(word_play,reverse_thinking):\n    final_result = {}\n    word_data_list = []\n    word_data_list = list(word_play.values())\n    print('#########Wordplay##########')\n    final_result['wordplay'] = getMetric(word_data_list)\n    \n    reverse_data_list = []\n    for item in reverse_thinking.values():\n        reverse_data_list.append(item)\n    print('#########Sentence##########')   \n    final_result['sentence'] = getMetric(reverse_data_list)  \n    \n    \n    all_data = word_data_list + reverse_data_list\n    print('#########All data##########') \n    final_result['all'] = getMetric(all_data) \n    \n    return final_result","metadata":{"_kg_hide-input":false,"execution":{"iopub.status.busy":"2024-05-27T19:47:46.067398Z","iopub.execute_input":"2024-05-27T19:47:46.067736Z","iopub.status.idle":"2024-05-27T19:47:46.086849Z","shell.execute_reply.started":"2024-05-27T19:47:46.067695Z","shell.execute_reply":"2024-05-27T19:47:46.086052Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"### Spremanje datoteka\n\nAutomatski dodaje +1 na naziv ako vec postoji takva datoteka\n\nUzima format model_name + name + redni broj za datoteku\n\nUnutra pise prvi testni primjer (example) i rezultate","metadata":{}},{"cell_type":"code","source":"def save(final_result, name=None, few_shot=False, example=format):\n    try:\n        os.mkdir('results')\n    except:\n        pass\n    model_name = model_path.split('/')[-1]\n    inserted_name = \"\" if name is None else \"_\" + name\n    i = 1\n    while True:\n        file_path = f'results/{model_name}{inserted_name}_{i}.txt'\n        try:\n            with open(file_path, 'r'):\n                i += 1\n        except:\n            with open(file_path, 'w') as file:\n                file.write(model_name+'\\n')\n                if few_shot:\n                    file.write(few_shot + '\\n')\n                if not isinstance(example, str):\n                    example = json.dumps(example)\n                file.write(example + '\\n')\n                file.write(json.dumps(final_result, indent=4))\n                return","metadata":{"execution":{"iopub.status.busy":"2024-05-27T19:47:46.087733Z","iopub.execute_input":"2024-05-27T19:47:46.087966Z","iopub.status.idle":"2024-05-27T19:47:46.100371Z","shell.execute_reply.started":"2024-05-27T19:47:46.087945Z","shell.execute_reply":"2024-05-27T19:47:46.099578Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"### Load the model\n\nLogin kako bi se mogli ucitati napredniji modeli","metadata":{}},{"cell_type":"code","source":"device = 'cuda' if torch.cuda.is_available() else 'cpu'\ndevice","metadata":{"execution":{"iopub.status.busy":"2024-05-27T19:47:46.101447Z","iopub.execute_input":"2024-05-27T19:47:46.101858Z","iopub.status.idle":"2024-05-27T19:47:46.137531Z","shell.execute_reply.started":"2024-05-27T19:47:46.101826Z","shell.execute_reply":"2024-05-27T19:47:46.136701Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"'cuda'"},"metadata":{}}]},{"cell_type":"markdown","source":"Logiranje u huggingiface\ntreba kljuc, njega nabavite na huggingface stranici","metadata":{}},{"cell_type":"code","source":"from huggingface_hub import login\nlogin('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', add_to_git_credential=False, write_permission=True)","metadata":{"execution":{"iopub.status.busy":"2024-05-27T19:47:46.138872Z","iopub.execute_input":"2024-05-27T19:47:46.139574Z","iopub.status.idle":"2024-05-27T19:47:46.239439Z","shell.execute_reply.started":"2024-05-27T19:47:46.139540Z","shell.execute_reply":"2024-05-27T19:47:46.238563Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\nToken is valid (permission: write).\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\n","output_type":"stream"}]},{"cell_type":"markdown","source":"BitsAndBytesConfig je novi, to kao kvantizira model pa stane na graficku","metadata":{}},{"cell_type":"code","source":"# !pip install ai2-olmo","metadata":{"execution":{"iopub.status.busy":"2024-05-27T19:47:46.240673Z","iopub.execute_input":"2024-05-27T19:47:46.240935Z","iopub.status.idle":"2024-05-27T19:47:46.244491Z","shell.execute_reply.started":"2024-05-27T19:47:46.240913Z","shell.execute_reply":"2024-05-27T19:47:46.243627Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"\n# model_path = 'google/flan-t5-xxl'\n# model_path = 'google/flan-t5-xl'\n# model_path = 'google/flan-t5-large'\n# model_path = 'mistralai/Mistral-7B-Instruct-v0.2'\n# model_path = 'mistralai/Mistral-7B-v0.3'\n# model_path = 'microsoft/phi-2'\n# model_path = 'microsoft/Phi-3-mini-128k-instruct'\n# model_path = 'microsoft/Phi-3-mini-4k-instruct'\n# model_path = '01-ai/Yi-1.5-6B'\n# model_path = 'netcat420/MFANN3bv0.6'\n# model_path = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n# model_path = \"meta-llama/Meta-Llama-3-8B\"\n# model_path = \"allenai/OLMo-7B-Instruct\"\n# model_path = \"tiiuae/falcon-7b-instruct\"\nmodel_path = \"tiiuae/falcon-7b\"\n# model_path = 'Intel/neural-chat-7b-v3-1'\n\nquantization_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_compute_dtype=torch.float16,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_use_double_quant=True,\n)\n\ndef get_mistral():\n    tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n    model = AutoModelForCausalLM.from_pretrained(\n        model_path,\n        device_map=\"auto\",\n        quantization_config=quantization_config,\n        trust_remote_code=True,\n    )\n    return tokenizer, model\n\ndef get_llama():\n    tokenizer = AutoTokenizer.from_pretrained(model_path)\n    model_4bit = AutoModelForCausalLM.from_pretrained(\n        model_path,\n#         \"SweatyCrayfish/llama-3-8b-quantized\", \n#         load_in_4bit=True, \n#         torch_dtype=torch.float16,\n        device_map=\"auto\",\n        quantization_config=quantization_config,\n    )\n    return tokenizer, model_4bit\n\ntokenizer, model = get_mistral()\n","metadata":{"execution":{"iopub.status.busy":"2024-05-27T19:47:46.245557Z","iopub.execute_input":"2024-05-27T19:47:46.245841Z","iopub.status.idle":"2024-05-27T19:51:24.169756Z","shell.execute_reply.started":"2024-05-27T19:47:46.245817Z","shell.execute_reply":"2024-05-27T19:51:24.168879Z"},"trusted":true},"execution_count":14,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/287 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c6ca5a9ee4154654af5e6755b2b94aa3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/2.73M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2f55b3096eac4f369f2333470c184fcb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/281 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2670ee86bbce45e98abd2994cf3ee006"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.05k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ec0374299dfa41c1bd3c2e7d2fca4d7b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"configuration_falcon.py:   0%|          | 0.00/7.16k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c5705588dca241e28bd348d6576f1dfe"}},"metadata":{}},{"name":"stderr","text":"A new version of the following files was downloaded from https://huggingface.co/tiiuae/falcon-7b:\n- configuration_falcon.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"modeling_falcon.py:   0%|          | 0.00/56.9k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7b727e5ffd404755accc7ec5e1c0bfc6"}},"metadata":{}},{"name":"stderr","text":"A new version of the following files was downloaded from https://huggingface.co/tiiuae/falcon-7b:\n- modeling_falcon.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin.index.json:   0%|          | 0.00/16.9k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e220864a26404f4fb1cd3b48faa8b9c2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a84345182e1649acace6b8e7e4767176"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model-00001-of-00002.bin:   0%|          | 0.00/9.95G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b596f5c4cabe4ea7ac5b510327b43db3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model-00002-of-00002.bin:   0%|          | 0.00/4.48G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cd033ca80074408ead6135c84aa5d7c2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d8c352273e8f4d10a641f25cab19e26d"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/117 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a1a3d7917603453384240aef56cf7a98"}},"metadata":{}}]},{"cell_type":"markdown","source":"### Dataset\n\n- Skinuti s [linka](https://drive.google.com/drive/u/0/folders/1kiFXp5fqpf8--NQJJAlBIBpfSaXTk1UY)\n\n- Staviti u folder `/data`, ako na Kaggleu, onda napraviti svoj dataset te ga dodati pod input","metadata":{}},{"cell_type":"code","source":"# few_shot_examples = list(np.load(\"./{dataset_path}/data/demonstration.npy\",allow_pickle=True))\ndata_path = '/kaggle/input/brainteaser/data'\nsentence_data_path = f\"{data_path}/SP-train.npy\"\nwordplay_data_list = f\"{data_path}/WP-train.npy\"\nsentence_data_list = list(np.load(sentence_data_path,allow_pickle=True))\nwordplay_data_list = list(np.load(wordplay_data_list,allow_pickle=True))\n\ntest_data_list = sentence_data_list + wordplay_data_list\nprint(f\"Dataset length {len(test_data_list)}\")","metadata":{"execution":{"iopub.status.busy":"2024-05-27T19:51:24.173008Z","iopub.execute_input":"2024-05-27T19:51:24.173306Z","iopub.status.idle":"2024-05-27T19:51:24.197324Z","shell.execute_reply.started":"2024-05-27T19:51:24.173280Z","shell.execute_reply":"2024-05-27T19:51:24.196471Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"Dataset length 903\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Format za mystral, za ostale modele maknuti linije `<s>[INST]`","metadata":{}},{"cell_type":"code","source":"format = \"\"\ndef get_format():\n    global format\n    newline = '\\n'\n    format = f\"\"\"Question: {\"{}\"}\nChoice:\n{''.join('(' + x + ') {}' + newline for x in letters)}\"\"\"\n    return format\n\ndef get_llama_format(demonstration=None):\n    global tokenizer, format\n    messages = []\n    if demonstration is not None:\n        messages.append({\"role\": \"system\", \"content\": demonstration})\n    messages.append({\"role\": \"user\", \"content\": get_format()})\n    messages.append({\"role\": \"assistant\", \"content\": \"Answer:(\"})\n    try:\n        format = tokenizer.apply_chat_template(messages, tokenize=False)\n    except Exception:\n        # When using mistral model, roles in messages must alternate, this way it works without errors\n        messages[1][\"content\"] = demonstration + \"\\n\\n\" + messages[1][\"content\"]\n        format = tokenizer.apply_chat_template(messages[1:], tokenize=False)\n    # remove \"<|eot_id|>\" from the end\n    format = format[:format.index(\"Answer:(\")+8]\n\ndef get_appended_format(demonstration=None):\n    global format\n    format = get_format()\n    if demonstration is not None:\n        format = demonstration + \"\\n\\n\" + format\n    format += \"\\nAnswer:(\"\n\nget_llama_format(\"Primjer\")\nprint(format)\nget_appended_format()","metadata":{"execution":{"iopub.status.busy":"2024-05-27T19:51:24.198251Z","iopub.execute_input":"2024-05-27T19:51:24.198506Z","iopub.status.idle":"2024-05-27T19:51:24.254936Z","shell.execute_reply.started":"2024-05-27T19:51:24.198483Z","shell.execute_reply":"2024-05-27T19:51:24.254080Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stderr","text":"No chat template is set for this tokenizer, falling back to a default class-level template. This is very error-prone, because models are often trained with templates different from the class default! Default chat templates are a legacy feature and will be removed in Transformers v4.43, at which point any code depending on them will stop working. We recommend setting a valid chat template before then to ensure that this model continues working without issues.\n","output_type":"stream"},{"name":"stdout","text":"<|im_start|>system\nPrimjer<|im_end|>\n<|im_start|>user\nQuestion: {}\nChoice:\n(A) {}\n(B) {}\n(C) {}\n(D) {}\n<|im_end|>\n<|im_start|>assistant\nAnswer:(\n","output_type":"stream"}]},{"cell_type":"code","source":"def get_single_demo(sample):\n    global format\n    sample_demo = format.format(\n        sample['question'], *sample['choice_list'])\n    return sample_demo\n\ngood_responses = [f'{x})' for x in letters]\nprint(\"good_responses\", good_responses)","metadata":{"execution":{"iopub.status.busy":"2024-05-27T19:51:24.255882Z","iopub.execute_input":"2024-05-27T19:51:24.256336Z","iopub.status.idle":"2024-05-27T19:51:24.261244Z","shell.execute_reply.started":"2024-05-27T19:51:24.256312Z","shell.execute_reply":"2024-05-27T19:51:24.260375Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"good_responses ['A)', 'B)', 'C)', 'D)']\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Generiranje dataseta\n\n### Pomocne funkcije","metadata":{}},{"cell_type":"code","source":"def set_predictions(data_list):\n    \"\"\"Uzima response i parsira ga tako da pod predict stavi slovo koje je napisano\"\"\"\n    for index,item in enumerate(data_list):\n        item['predict'] = None\n        for x in letters:\n            if (f'{x})') in item['response']:\n                item['predict'] = x\n\n        if item['predict'] is None:\n            print(index)\n\ndef custom_stopping_criteria(input_ids: torch.LongTensor, score: torch.FloatTensor, **kwargs) -> bool:\n    \"\"\"\n    Funkcija kako bi se dinamicki prepoznalo kad treba prestati generirati tekst:\n    ne nakon fiksnog broja tokena, vec kad model napise rjesenje. Najcesce\n    ce to ipak biti medu prva dva tokena\n    \"\"\"\n    decoded = tokenizer.decode(input_ids[0][-3:])\n    for good_response in good_responses:\n        if good_response in decoded:\n            return True\n    return False\n\nstopping_criteria = StoppingCriteriaList([custom_stopping_criteria])","metadata":{"execution":{"iopub.status.busy":"2024-05-27T19:51:24.262352Z","iopub.execute_input":"2024-05-27T19:51:24.262641Z","iopub.status.idle":"2024-05-27T19:51:24.270592Z","shell.execute_reply.started":"2024-05-27T19:51:24.262609Z","shell.execute_reply":"2024-05-27T19:51:24.269719Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"def generate(samples, tokens=10, all_tokens=True, few_shot=False):\n    global model\n    for sample in tqdm(samples):\n        if few_shot:\n            text = demonstration + get_single_demo(sample)\n        else:\n            text = get_single_demo(sample)\n        inputs = tokenizer.encode(text, return_tensors=\"pt\", return_attention_mask=False).to(device)\n        original_tokens = len(inputs[0])\n        outputs = model.generate(\n            inputs,\n            pad_token_id=tokenizer.eos_token_id,\n            do_sample = False,\n            max_new_tokens=tokens,\n            stopping_criteria=stopping_criteria\n        )\n        outputs = outputs[0][0 if all_tokens else original_tokens:]\n        sample['response'] = tokenizer.decode(outputs)\n    set_predictions(samples)","metadata":{"execution":{"iopub.status.busy":"2024-05-27T19:51:24.271754Z","iopub.execute_input":"2024-05-27T19:51:24.272112Z","iopub.status.idle":"2024-05-27T19:51:24.284875Z","shell.execute_reply.started":"2024-05-27T19:51:24.272057Z","shell.execute_reply":"2024-05-27T19:51:24.283997Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"## Testiranje","metadata":{}},{"cell_type":"code","source":"get_llama_format()\ngenerate(test_data_list[:10], tokens=1000, all_tokens=False, few_shot=False)","metadata":{"execution":{"iopub.status.busy":"2024-05-27T19:51:24.285893Z","iopub.execute_input":"2024-05-27T19:51:24.286265Z","iopub.status.idle":"2024-05-27T19:51:30.035653Z","shell.execute_reply.started":"2024-05-27T19:51:24.286230Z","shell.execute_reply":"2024-05-27T19:51:30.034799Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stderr","text":"100%|██████████| 10/10 [00:05<00:00,  1.74it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"for sample in test_data_list[:10]:\n    print(\"\\n>\", sample['response'])","metadata":{"execution":{"iopub.status.busy":"2024-05-27T19:51:30.036902Z","iopub.execute_input":"2024-05-27T19:51:30.037236Z","iopub.status.idle":"2024-05-27T19:51:30.041949Z","shell.execute_reply.started":"2024-05-27T19:51:30.037175Z","shell.execute_reply":"2024-05-27T19:51:30.041060Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"\n> A)\n\n> A)\n\n> A)\n\n> A)\n\n> A)\n\n> A)\n\n> A)\n\n> A)\n\n> A)\n\n> A)\n","output_type":"stream"}]},{"cell_type":"code","source":"get_llama_format()\ntext = get_single_demo(test_data_list[1])\ninputs = tokenizer.encode(text, return_tensors=\"pt\", return_attention_mask=False).to(device)\noutputs = model.generate(\n    inputs,\n    pad_token_id=tokenizer.eos_token_id,\n    do_sample = False,\n    max_new_tokens=50,\n)\nprint(tokenizer.decode(outputs[0]))","metadata":{"execution":{"iopub.status.busy":"2024-05-27T19:51:30.043152Z","iopub.execute_input":"2024-05-27T19:51:30.043451Z","iopub.status.idle":"2024-05-27T19:51:33.450799Z","shell.execute_reply.started":"2024-05-27T19:51:30.043426Z","shell.execute_reply":"2024-05-27T19:51:33.449822Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"<|im_start|>user\nQuestion: The six daughters of Mr. and Mrs. Mustard each have one brother. However, the family only consists of nine people; how is that possible?\nChoice:\n(A) Some brothers were not loved by family and moved away.\n(B) Some daughters get married and have their own family.\n(C) Each daughter shares the same brother.\n(D) None of above.\n<|im_end|>\n<|im_start|>assistant\nAnswer:(A) Some brothers were not loved by family and moved away.\n<|im_end|>\n<|im_start|>assistant\nAnswer:(B) Some daughters get married and have their own family.\n<|im_end\n","output_type":"stream"}]},{"cell_type":"code","source":"# generate(test_data_list, tokens=20, all_tokens=False, few_shot=False)\n\n# word_play,sentence_play = getResultdata(test_data_list)\n# # word_play,sentence_play = getResultdata(test_data_list)\n# final_result = getSeperateResult(word_play, sentence_play)\n\n# save(final_result, name=\"with-instruction\", example=get_single_demo(test_data_list[0]))","metadata":{"execution":{"iopub.status.busy":"2024-05-27T19:51:33.452054Z","iopub.execute_input":"2024-05-27T19:51:33.452396Z","iopub.status.idle":"2024-05-27T19:51:33.458756Z","shell.execute_reply.started":"2024-05-27T19:51:33.452352Z","shell.execute_reply":"2024-05-27T19:51:33.457826Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"demonstration1 = \"Please pick the best choice for the brain teaser. Each brain teaser has only one possible solution including the choice none of above, answer should only provide the choice:\"\ndemonstration2 = \"The following question is a brainteaser. One choice is correct, other choices are semanticaly derived from the question.\"\ndemonstration3 = \"The following question is a brainteaser.\"\ndemonstration4 = \"The following question is a brainteaser. One choice is correct, other choices are semanticaly derived from the question. Sometimes none of the above is correct.\"\ndemonstration5 = \"The following question is a brainteaser. Sometimes none of the above is correct.\"","metadata":{"execution":{"iopub.status.busy":"2024-05-27T19:51:33.459840Z","iopub.execute_input":"2024-05-27T19:51:33.460333Z","iopub.status.idle":"2024-05-27T19:51:33.467208Z","shell.execute_reply.started":"2024-05-27T19:51:33.460282Z","shell.execute_reply":"2024-05-27T19:51:33.466338Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":"\n\n- `The following question is a brainteaser. One choice is correct, other choices are semanticaly derived from the question. Sometimes none of the above is correct.`\n\n- `The following question is a brainteaser. One choice is correct, other choices are semanticaly derived from the question.`\n\n- `The following question is a brainteaser.` (less informative, helpful) \n\n- `Please pick the best choice for the brain teaser. Each brain teaser has only one possible solution including the choice none of above, answer should only provide the choice:` (Original, more informative, helpful) ","metadata":{"execution":{"iopub.status.busy":"2024-05-24T12:31:13.722785Z","iopub.execute_input":"2024-05-24T12:31:13.723172Z","iopub.status.idle":"2024-05-24T12:31:14.068435Z","shell.execute_reply.started":"2024-05-24T12:31:13.723141Z","shell.execute_reply":"2024-05-24T12:31:14.067422Z"}}},{"cell_type":"code","source":"def generate_and_save(test_data_list, name=\"normal\"):\n    generate(test_data_list, tokens=20, all_tokens=False, few_shot=False)\n\n    word_play,sentence_play = getResultdata(test_data_list)\n    # word_play,sentence_play = getResultdata(test_data_list)\n    final_result = getSeperateResult(word_play, sentence_play)\n\n    save(final_result, name=name, example=get_single_demo(test_data_list[0]))","metadata":{"execution":{"iopub.status.busy":"2024-05-27T19:51:33.468424Z","iopub.execute_input":"2024-05-27T19:51:33.468688Z","iopub.status.idle":"2024-05-27T19:51:33.477359Z","shell.execute_reply.started":"2024-05-27T19:51:33.468664Z","shell.execute_reply":"2024-05-27T19:51:33.476433Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"def generate_with_demonstration(demonstration=None,test_data_list=test_data_list, name=\"with-instruction\", llama_format=False):\n    if llama_format:\n        get_llama_format(demonstration)\n    else:\n        get_appended_format(demonstration)\n    generate_and_save(test_data_list, name=name)","metadata":{"execution":{"iopub.status.busy":"2024-05-27T19:51:33.480402Z","iopub.execute_input":"2024-05-27T19:51:33.480658Z","iopub.status.idle":"2024-05-27T19:51:33.486346Z","shell.execute_reply.started":"2024-05-27T19:51:33.480635Z","shell.execute_reply":"2024-05-27T19:51:33.485593Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"get_llama_format(\"jalo\")\nprint(format[:format.index(\"Answer:(\")+8])","metadata":{"execution":{"iopub.status.busy":"2024-05-27T19:51:33.487596Z","iopub.execute_input":"2024-05-27T19:51:33.487950Z","iopub.status.idle":"2024-05-27T19:51:33.500682Z","shell.execute_reply.started":"2024-05-27T19:51:33.487910Z","shell.execute_reply":"2024-05-27T19:51:33.499846Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stdout","text":"<|im_start|>system\njalo<|im_end|>\n<|im_start|>user\nQuestion: {}\nChoice:\n(A) {}\n(B) {}\n(C) {}\n(D) {}\n<|im_end|>\n<|im_start|>assistant\nAnswer:(\n","output_type":"stream"}]},{"cell_type":"code","source":"This question is a brainteaser.\n\nQuestion: {}\nChoice:\n(A) {}\n(B) {}\n(C) {}\n(D) {}\n\nAnswer:(","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# save({}, name=\"normal\", example=get_single_demo(test_data_list[0]))","metadata":{"execution":{"iopub.status.busy":"2024-05-27T19:51:33.501691Z","iopub.execute_input":"2024-05-27T19:51:33.501923Z","iopub.status.idle":"2024-05-27T19:51:33.509317Z","shell.execute_reply.started":"2024-05-27T19:51:33.501902Z","shell.execute_reply":"2024-05-27T19:51:33.508499Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"# generate_with_demonstration(test_data_list[:10]+ test_data_list[-10:], name=\"normal\")","metadata":{"execution":{"iopub.status.busy":"2024-05-27T19:51:33.510253Z","iopub.execute_input":"2024-05-27T19:51:33.510516Z","iopub.status.idle":"2024-05-27T19:51:33.518497Z","shell.execute_reply.started":"2024-05-27T19:51:33.510493Z","shell.execute_reply":"2024-05-27T19:51:33.517721Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"# generate_with_demonstration(name=\"normal\")\n# generate_with_demonstration(demonstration1, name=\"dem1\")\n# generate_with_demonstration(demonstration3, name=\"dem3\")\n# generate_with_demonstration(demonstration2, name=\"dem2\")\n# generate_with_demonstration(demonstration4, name=\"dem4\")\n\n# generate_with_demonstration(name=\"normal-spec\")\n# generate_with_demonstration(demonstration1, name=\"dem1-spec\", llama_format=True)\n# generate_with_demonstration(demonstration3, name=\"dem3-spec\", llama_format=True)\n# generate_with_demonstration(demonstration2, name=\"dem2-spec\", llama_format=True)\n# generate_with_demonstration(demonstration4, name=\"dem4-spec\", llama_format=True)","metadata":{"execution":{"iopub.status.busy":"2024-05-27T19:51:33.519387Z","iopub.execute_input":"2024-05-27T19:51:33.519856Z","iopub.status.idle":"2024-05-27T19:51:33.528496Z","shell.execute_reply.started":"2024-05-27T19:51:33.519831Z","shell.execute_reply":"2024-05-27T19:51:33.527743Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"test_data_list[0]","metadata":{"execution":{"iopub.status.busy":"2024-05-27T19:51:33.529356Z","iopub.execute_input":"2024-05-27T19:51:33.529583Z","iopub.status.idle":"2024-05-27T19:51:33.540842Z","shell.execute_reply.started":"2024-05-27T19:51:33.529562Z","shell.execute_reply":"2024-05-27T19:51:33.539968Z"},"trusted":true},"execution_count":31,"outputs":[{"execution_count":31,"output_type":"execute_result","data":{"text/plain":"{'id': 'SP-0',\n 'question': 'Mr. and Mrs. Mustard have six daughters and each daughter has one brother. But there are only 9 people in the family, how is that possible?',\n 'answer': 'Each daughter shares the same brother.',\n 'distractor1': 'Some daughters get married and have their own family.',\n 'distractor2': 'Some brothers were not loved by family and moved away.',\n 'distractor(unsure)': 'None of above.',\n 'label': 1,\n 'choice_list': ['Some daughters get married and have their own family.',\n  'Each daughter shares the same brother.',\n  'Some brothers were not loved by family and moved away.',\n  'None of above.'],\n 'choice_order': [1, 0, 2, 3],\n 'response': 'A)',\n 'predict': 'A'}"},"metadata":{}}]},{"cell_type":"code","source":"model = None\ndef run_with_model(model_path_tmp):\n    global tokenizer, model, model_path\n    del model\n    model = None\n    gc.collect()\n    torch.cuda.empty_cache()\n    model_path = model_path_tmp\n    print(f\"Loading {model_path}\")\n    tokenizer, model = get_mistral()\n\n    print(f\"Running {model_path}\")\n    generate_with_demonstration(demonstration5, name=\"dem5\")\n    if 'instruct' in model_path or 'Instruct' in model_path:\n        generate_with_demonstration(demonstration5, name=\"dem5-spec\", llama_format=True)","metadata":{"execution":{"iopub.status.busy":"2024-05-27T19:51:33.541963Z","iopub.execute_input":"2024-05-27T19:51:33.542311Z","iopub.status.idle":"2024-05-27T19:51:33.548773Z","shell.execute_reply.started":"2024-05-27T19:51:33.542279Z","shell.execute_reply":"2024-05-27T19:51:33.547997Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"\nmodels = [\n#     'mistralai/Mistral-7B-Instruct-v0.2',\n#     'mistralai/Mistral-7B-v0.3',\n#     \"meta-llama/Meta-Llama-3-8B-Instruct\",\n    \"meta-llama/Meta-Llama-3-8B\",\n#     \"allenai/OLMo-7B-Instruct\",\n    \"tiiuae/falcon-7b-instruct\",\n    \"tiiuae/falcon-7b\",\n]\n\nfor model_path_tmp in models:\n    run_with_model(model_path_tmp)","metadata":{"execution":{"iopub.status.busy":"2024-05-27T19:51:33.552895Z","iopub.execute_input":"2024-05-27T19:51:33.553640Z","iopub.status.idle":"2024-05-27T20:28:00.362932Z","shell.execute_reply.started":"2024-05-27T19:51:33.553614Z","shell.execute_reply":"2024-05-27T20:28:00.361001Z"},"trusted":true},"execution_count":33,"outputs":[{"name":"stdout","text":"Loading meta-llama/Meta-Llama-3-8B\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/50.6k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6d23e48d1d1b4469b916268bcd1e7d9c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f519317aea934cd5ac12f72cb1a4d810"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/73.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5c8a8ed7ccb649a0b1008be1e2d90f8c"}},"metadata":{}},{"name":"stderr","text":"Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/654 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7e5388fc4d674c94b1208f54d5cbf25e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f3df9f614f754919b65a02e39ab47c1d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"43007290689e43c9ba5f46423c9c6b2d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e760858f6f6f49a2b2848c4e82e233d7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"73a5b29588584f34a042328dd7dee27a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ab9b752c7b1446ffbe2c6b946ff28ad5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00004-of-00004.safetensors:   0%|          | 0.00/1.17G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b1a50c579e2d453abbc868d31c919529"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2158f24986c54e9a808817d496b83903"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/177 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c2368ca81408430690562fefc36edc6a"}},"metadata":{}},{"name":"stdout","text":"Running meta-llama/Meta-Llama-3-8B\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/903 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:515: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:520: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n  warnings.warn(\n100%|██████████| 903/903 [06:54<00:00,  2.18it/s]\n","output_type":"stream"},{"name":"stdout","text":"#########Wordplay##########\nover_all accuracy 0.15151515151515152\nsingle_original_accuracy 0.12878787878787878\nsingle_semantic_accuracy 0.13636363636363635\nsingle_context_accuracy 0.1893939393939394\nsr_accuracy 0.045454545454545456\ncr_accuracy 0.007575757575757576\n#########Sentence##########\nover_all accuracy 0.25641025641025644\nsingle_original_accuracy 0.28994082840236685\nsingle_semantic_accuracy 0.2485207100591716\nsingle_context_accuracy 0.23076923076923078\nsr_accuracy 0.1952662721893491\ncr_accuracy 0.10650887573964497\n#########All data##########\nover_all accuracy 0.21040974529346623\nsingle_original_accuracy 0.21926910299003322\nsingle_semantic_accuracy 0.19933554817275748\nsingle_context_accuracy 0.21262458471760798\nsr_accuracy 0.12956810631229235\ncr_accuracy 0.06312292358803986\nLoading tiiuae/falcon-7b-instruct\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/287 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5c271856f57e44d086d47734c7f35a83"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/2.73M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"44d267469fe840a8b6f8e820fa2171f0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/281 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bf21b35eba4140f0b087090d13de7685"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.05k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fd821e99b6ab4d7a91faab86ca820aa7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"configuration_falcon.py:   0%|          | 0.00/7.16k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"824356be36f44400b767ae54d1e39cfa"}},"metadata":{}},{"name":"stderr","text":"A new version of the following files was downloaded from https://huggingface.co/tiiuae/falcon-7b-instruct:\n- configuration_falcon.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"modeling_falcon.py:   0%|          | 0.00/56.9k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"484efbabeb0a4ad480bf7899da7a27c8"}},"metadata":{}},{"name":"stderr","text":"A new version of the following files was downloaded from https://huggingface.co/tiiuae/falcon-7b-instruct:\n- modeling_falcon.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin.index.json:   0%|          | 0.00/16.9k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"81dafeea22aa47fd8c72628a0ab7154b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1e6a300d582c46c0b13feb3cc208a7ec"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model-00001-of-00002.bin:   0%|          | 0.00/9.95G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5668d5a074d6403f8157067a534c48ef"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model-00002-of-00002.bin:   0%|          | 0.00/4.48G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8269e7ccd740420f870cff61fcb676e7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d08b35d57a234f2d824d2b0956057b39"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/117 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a411cc200ac84cbb90d5294ae3b956e7"}},"metadata":{}},{"name":"stdout","text":"Running tiiuae/falcon-7b-instruct\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 903/903 [07:08<00:00,  2.11it/s]\n","output_type":"stream"},{"name":"stdout","text":"#########Wordplay##########\nover_all accuracy 0.3484848484848485\nsingle_original_accuracy 0.3333333333333333\nsingle_semantic_accuracy 0.3484848484848485\nsingle_context_accuracy 0.36363636363636365\nsr_accuracy 0.12121212121212122\ncr_accuracy 0.03787878787878788\n#########Sentence##########\nover_all accuracy 0.30177514792899407\nsingle_original_accuracy 0.3431952662721893\nsingle_semantic_accuracy 0.2781065088757396\nsingle_context_accuracy 0.28402366863905326\nsr_accuracy 0.10059171597633136\ncr_accuracy 0.03550295857988166\n#########All data##########\nover_all accuracy 0.3222591362126246\nsingle_original_accuracy 0.3388704318936877\nsingle_semantic_accuracy 0.3089700996677741\nsingle_context_accuracy 0.31893687707641194\nsr_accuracy 0.10963455149501661\ncr_accuracy 0.036544850498338874\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 903/903 [08:18<00:00,  1.81it/s]\n","output_type":"stream"},{"name":"stdout","text":"#########Wordplay##########\nover_all accuracy 0.34595959595959597\nsingle_original_accuracy 0.3181818181818182\nsingle_semantic_accuracy 0.3409090909090909\nsingle_context_accuracy 0.3787878787878788\nsr_accuracy 0.10606060606060606\ncr_accuracy 0.03787878787878788\n#########Sentence##########\nover_all accuracy 0.30177514792899407\nsingle_original_accuracy 0.3431952662721893\nsingle_semantic_accuracy 0.2781065088757396\nsingle_context_accuracy 0.28402366863905326\nsr_accuracy 0.10059171597633136\ncr_accuracy 0.03550295857988166\n#########All data##########\nover_all accuracy 0.32115171650055374\nsingle_original_accuracy 0.33222591362126247\nsingle_semantic_accuracy 0.30564784053156147\nsingle_context_accuracy 0.32558139534883723\nsr_accuracy 0.10299003322259136\ncr_accuracy 0.036544850498338874\nLoading tiiuae/falcon-7b\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cce0ded984234fdd8d521be94f6611ca"}},"metadata":{}},{"name":"stdout","text":"Running tiiuae/falcon-7b\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 903/903 [07:08<00:00,  2.11it/s]","output_type":"stream"},{"name":"stdout","text":"#########Wordplay##########\nover_all accuracy 0.31313131313131315\nsingle_original_accuracy 0.3333333333333333\nsingle_semantic_accuracy 0.32575757575757575\nsingle_context_accuracy 0.2803030303030303\nsr_accuracy 0.15151515151515152\ncr_accuracy 0.045454545454545456\n#########Sentence##########\nover_all accuracy 0.28205128205128205\nsingle_original_accuracy 0.26627218934911245\nsingle_semantic_accuracy 0.2781065088757396\nsingle_context_accuracy 0.30177514792899407\nsr_accuracy 0.0650887573964497\ncr_accuracy 0.01775147928994083\n#########All data##########\nover_all accuracy 0.2956810631229236\nsingle_original_accuracy 0.2956810631229236\nsingle_semantic_accuracy 0.29900332225913623\nsingle_context_accuracy 0.292358803986711\nsr_accuracy 0.10299003322259136\ncr_accuracy 0.029900332225913623\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"code","source":"1","metadata":{"execution":{"iopub.status.busy":"2024-05-27T20:28:00.365725Z","iopub.execute_input":"2024-05-27T20:28:00.365989Z","iopub.status.idle":"2024-05-27T20:28:00.373045Z","shell.execute_reply.started":"2024-05-27T20:28:00.365964Z","shell.execute_reply":"2024-05-27T20:28:00.372120Z"},"trusted":true},"execution_count":34,"outputs":[{"execution_count":34,"output_type":"execute_result","data":{"text/plain":"1"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}